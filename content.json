{"posts":[{"title":"README","text":"Hello 大家好我是Codedm，这个仓库主要是和大家分享一些关于Java生态的技术。主要包括Java 基础、Java 进阶、Flink、大数据、算法、Java Web等等 文章导航 post文件夹：所有技术文章，子文件夹按照文章类型分类 src：关于文章中出现的源码 文章特点 每篇文章都会有对应的 issues：issues中记录了我在写文章的过程，做了哪些POC，参考的文章连接。 文章发布过程大家可以通过这个连接查看这个仓库的历史创作以及未来创作。https://github.com/users/code-dm/projects/1 博客地址https://codedm.com/ 关注我 添加我的微信","link":"/posts/77cd4175.html"},{"title":"Docker安装MySQL并开启BinLog","text":"确保已经安装Docker适用于 MySQL5.7 创建配置文件创建一个配置文件目录，并新建一个my.cnf文件，并插入下面内容： 12345678910111213141516171819202122232425262728293031323334## The MySQL Server configuration file.## For explanations see# http://dev.mysql.com/doc/mysql/en/server-system-variables.html[mysqld]pid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockdatadir = /var/lib/mysqlsecure-file-priv= NULLdefault-time-zone = '+8:00'#最大链接数max_connections=1024#是否对sql语句大小写敏感，1表示不敏感lower_case_table_names=1log_bin_trust_function_creators=1server-id=1000#启用log-binlog-bin=mysql-bin#设置日志格式binlog-format=Row#设置binlog清理时间expire_logs_days=7# 数据表默认时区default-time-zone='+08:00'# Custom config should go here!includedir /etc/mysql/conf.d/ 执行启动命令123456docker run -p 3308:3306 \\--name mysql-binlog \\-v 修改成你本机绝对路径/my.cnf:/etc/mysql/my.cnf \\-e MYSQL_ROOT_PASSWORD=123456 \\-d \\mysql:5.7.34 如果您本地没有mysql:5.7.34这个镜像，启动的时候会自动拉取这个镜像，拉取时间根据您的网速决定。","link":"/posts/5c6bc735.html"},{"title":"Flink Hive Connector 实战使用","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) 本文基于：Flink-1.15 准备工作下载 Flink 安装包1wget https://dlcdn.apache.org/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz --no-check-certificate 下载 Hive 依赖包选择一个下载并放在 Flink 解压包的/lib/ 目录中。 Metastore version Maven dependency SQL Client JAR 1.0.0 - 1.2.2 flink-sql-connector-hive-1.2.2 Download 2.0.0 - 2.2.0 flink-sql-connector-hive-2.2.0 Download 2.3.0 - 2.3.6 flink-sql-connector-hive-2.3.6 Download 3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Download Hadoop 依赖1export HADOOP_CLASSPATH=`hadoop classpath` 配置 Kerberos如果 Hive 开启的 Kerberos 则进行配置，如果没有则跳过。在 conf/flink-conf.yam 配置文件中增加以下配置。 123security.kerberos.login.use-ticket-cache: truesecurity.kerberos.login.keytab: /keytabs/hive.service.keytabsecurity.kerberos.login.principal: hive 命令行执行： 1kinit -kt xxx.keytab xxxx 启动一个 Flin Standalone 环境为了可以远程访问 Flink Web界面，你可以将 conf/flink-conf.yam 中所有 localhost 全部替换成 0.0.0.0。增加指定端口配置： 1rest.port: 8081 启动集群： 1bin/start-cluster.sh 如果集群启动成功，通过浏览器可以访问Flink Web界面。注意Available Task Slots必须 &gt;= 1，如果为 0，则启动有异常。 连接 Hive Catalog1bin/sql-client.sh 12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'yyq', 'hive-conf-dir' = '/etc/hive/conf'); CREATE CATALOG的一些详细配置：输入 show databases; 可以显示 Hive 中所有的数据库。也可以写个 SELECT，sql-client会将作业提交到 Standalone 集群中，并返回查询结果。 参数 必选 默认值 类型 描述 type 是 (无) String Catalog 的类型。 创建 HiveCatalog 时，该参数必须设置为’hive’。 name 是 (无) String Catalog 的名字。仅在使用 YAML file 时需要指定。 hive-conf-dir 否 (无) String 指向包含 hive-site.xml 目录的 URI。 该 URI 必须是 Hadoop 文件系统所支持的类型。 如果指定一个相对 URI，即不包含 scheme，则默认为本地文件系统。如果该参数没有指定，我们会在 class path 下查找hive-site.xml。 default-database 否 default String 当一个catalog被设为当前catalog时，所使用的默认当前database。 hive-version 否 (无) String HiveCatalog 能够自动检测使用的 Hive 版本。我们建议不要手动设置 Hive 版本，除非自动检测机制失败。 hadoop-conf-dir 否 (无) String Hadoop 配置文件目录的路径。目前仅支持本地文件系统路径。我们推荐使用 HADOOP_CONF_DIR 环境变量来指定 Hadoop 配置。因此仅在环境变量不满足您的需求时再考虑使用该参数，例如当您希望为每个 HiveCatalog 单独设置 Hadoop 配置时。 从MySQL中同步数据到Hive12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576CREATE TABLE IF NOT EXISTS test_partition( id string comment '字段id注释', name string comment '字段name注释') COMMENT '备注表名' PARTITIONED BY( `dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( &quot;orc.compress&quot;=&quot;SNAPPY&quot;);set hive.exec.dynamic.partition.mode=nonstrict;insert into table yyq.test_partition partition(dt='2022-09-20')values('4','test_partition');CREATE TABLE IF NOT EXISTS test_stream_orc( id string comment 'id', name string comment 'name') COMMENT 'test_stream' ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( &quot;orc.compress&quot;=&quot;SNAPPY&quot;);insert into table test_stream_orc values('1','name1');CREATE TABLE `myhive`.`yyq`.`test_partition` ( `id` VARCHAR(2147483647), `name` VARCHAR(2147483647), `dt` VARCHAR(2147483647)) PARTITIONED BY (`dt`)WITH ( 'orc.compress' = 'SNAPPY', 'transient_lastDdlTime' = '1663580859', 'bucketing_version' = '2', 'connector' = 'hive')select * from test_partition /*+ OPTIONS('streaming-source.enable'='true','streaming-source.monitor-interval'='10s') */;CREATE TABLE IF NOT EXISTS order_orc( order_id int, order_date Timestamp, customer_name string, price decimal(10, 5), product_id int, order_status BOOLEAN) COMMENT 'order_orc' PARTITIONED BY( `order_date_year` string, `order_date_month` string, `order_date_day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( &quot;orc.compress&quot;=&quot;SNAPPY&quot;);","link":"/posts/e220ca62.html"},{"title":"Flink Hive Connector 简介","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) 本文基于：Flink-1.15 简介什么是Hive如果是了解过Hive以及Flink，这些简介请直接跳过。 Apache Hive官网的描述 The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. Apache Hive是数据仓库的一种工具，用来读、写和管理分布式存储（例如：HDFS）中的大型的数据集。他可以把结构映射到存储的文件上。可以通过命令行工具和JDBC驱动来连接Hive。数据仓库收集企业中各个业务系统的数据进行集中化管理，Apache Hive可以作为数据仓库管理的一种工具。同时可以知道Hive本身是不存储数据的，他管理的是分布式存储系统中文件和结构的映射关系。例如：HDFS上有一个CSV文件，这个CSV文件使用 , 号进行分割 123a1,a2,a3b1,b2,b3c1,c2,c3 Apache Hive可以对这个CSV文件进行格式化，定义它的结构，例如定义分隔符、每一列的名称。定义完成后就可以使用SQL语句进行查询。这个过程可以看做是结构化的过程。 简介什么是Apache Flink相对于Apache Hive来说 Apache Flink则是Apache Hive的一个执行计算引擎。常见的Hive计算引擎有MapReduce、Spark，Flink也可以作为Hive的计算引擎。 Apache Flink官网的描述 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink是对无界和有界数据流进行计算的一个框架和分布式处理的引擎。Flink可以以内存中的运算速度和任何的规模在常见的集群中运行。 Hive Connector的简介存储、计算、管理都是数据仓库生态中很重要的一部分。Hive有作为数据仓库生态中的核心，所以Flink针对Hive做了很多适配工作： 适配 Hive 的 MetaStore利用了 Hive 的 MetaStore 作为持久化的 Catalog，用户可通过HiveCatalog将不同会话中的 Flink 元数据存储到 Hive Metastore 中。 例如，用户可以使用HiveCatalog将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。 适配Hive方言，可以在Flink SQL-Client或者Flink SQL-Gateway中使用Hive的方言。 适配Hive读写HiveCatalog的设计提供了与 Hive 良好的兼容性，用户可以”开箱即用”的访问其已有的 Hive 数仓。 您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。 适配Hive的函数 等等","link":"/posts/f3b4a32a.html"},{"title":"Flink Hive Connector创建Catalog的原理","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) 本文基于：Flink-1.15 简介在sql-client中使用Hive Connector之前常规都是需要将Flink和Hive Metastore连接起来，从Hive Metastore中获取Hive的元数据，方便我们直接操作Hive中的一些表。同时我们在sql-client中创建对表也可以持久化在Hive Metastore中。使用Flink HiveCatalog可以连接到Hive Metastore，这边文章主要来了解HiveCatalog的实现。 HiveCatalog12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'mydatabase', 'hive-conf-dir' = '/opt/hive-conf'); 使用CREATE CATALOG语句Flink Table会根据type使用SPI找到对应的工厂类：HiveCatalogFactory。HiveCatalogFactory主要作用是获取相关配置，并创建HiveCatalog类。此处创建的Catalog会被管理在CatalogManager类中。HiveCatalog中的一些方法 getDatabase、createDatabase、alterDatabase、listDatabase、、、 getTable、createTable、renameTable、alterTable、dropTable、、、 partitionExists、createPartition、、、 createFunction、alterFunction、、、 alterTableStatistics、alterPartitionStatistics、、、 、、、 通过这些方法我们可以增删改查Hive中的数据库、表、视图以及函数等等。 HiveShimHiveCatalog并不是直接连接Hive Metastore的，而是通过 HiveShim 来连接到Hive Metastore的。HiveShim 的作用是处理Flink连接 Hive 在不同 Hive 版本版本中 Hive Metastore 不兼容的问题。 作业执行流程提交创建Catalog语句12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'mydatabase', 'hive-conf-dir' = '/opt/hive-conf'); 用户通过 SQL Client 提交CREATE CATALOG SQL请求，Flink 会创建 TableEnvironment， TableEnvironment 会创建 CatalogManager 加载并配置 HiveCatalog 实例。初始化HiveCatalog会获取Hive的版本。后续会使用到。 提交USE语句1USE CATALOG myhive; CatalogManager 会修改全局currentCatalogName属性为myhive。 SHOW 语句1SHOW DATABASES; Flink Table 会调用HiveCatalog的listDatabases方法，Hive大版本不会存在差异的接口会使用IMetaStoreClient client直接获取getAllDatabases。 getHiveMetastoreClientgetHiveMetastoreClient在Hive各个大版本初始化存在差异，Flink使用HiveShim来处理不同版本的不同适配代码。例如：Hive1.0.0初始化HiveMetastoreClient代码： 12345678@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { return new HiveMetaStoreClient(hiveConf); } catch (MetaException ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }} Hive2.0.0初始化HiveMetastoreClient代码： 123456789101112131415161718192021222324@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { Class&lt;?&gt;[] constructorArgTypes = {HiveConf.class}; Object[] constructorArgs = {hiveConf}; Method method = RetryingMetaStoreClient.class.getMethod( &quot;getProxy&quot;, HiveConf.class, constructorArgTypes.getClass(), constructorArgs.getClass(), String.class); // getProxy is a static method return (IMetaStoreClient) method.invoke( null, hiveConf, constructorArgTypes, constructorArgs, HiveMetaStoreClient.class.getName()); } catch (Exception ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }} Hive3.1.0初始化HiveMetastoreClient代码： 123456789101112@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { Method method = RetryingMetaStoreClient.class.getMethod( &quot;getProxy&quot;, Configuration.class, Boolean.TYPE); // getProxy is a static method return (IMetaStoreClient) method.invoke(null, hiveConf, true); } catch (Exception ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }}","link":"/posts/31388dbe.html"},{"title":"Flink Hive Connector读写源码解析","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我)Flink实现Hive中表数据的读写操作首先要获取到每张表的元数据，通过表的元数据信息才能获取到数据文件的存储路径、存储格式、是否分区、分区规则。这样才能从HDFS对应的路径中获取到对应的数据，并使用对应的存储格式来解析文件。Flink Hive的读写同样使用Connector通用架构新Source（FLIP-27）和Sink。 Flink 读取 Hive 配置信息Flink 支持以批和流两种模式从 Hive 表中读取数据。批读的时候，Flink 会基于执行查询时表的状态进行查询。流读时将持续监控表，并在表中新数据可用时进行增量获取，默认情况下，Flink 将以批模式读取数据。流读支持消费分区表和非分区表。对于分区表，Flink 会监控新分区的生成，并且在数据可用的情况下增量获取数据。对于非分区表，Flink 将监控文件夹中新文件的生成，并增量地读取新文件。 键 默认值 类型 描述 streaming-source.enable false Boolean 是否启动流读。注意：请确保每个分区/文件都应该原子地写入，否则读取不到完整的数据。 streaming-source.partition.include all String 选择读取的分区，可选项为 all 和 latest，all 读取所有分区；latest 读取按照 ‘streaming-source.partition.order’ 排序后的最新分区，latest 仅在流模式的 Hive 源表作为时态表时有效。默认的选项是 all。在开启 ‘streaming-source.enable’ 并设置 ‘streaming-source.partition.include’ 为 ‘latest’ 时，Flink 支持 temporal join 最新的 Hive 分区，同时，用户可以通过配置分区相关的选项来配置分区比较顺序和数据更新时间间隔。 streaming-source.monitor-interval None Duration 连续监控分区/文件的时间间隔。 注意: 默认情况下，流式读 Hive 的间隔为 ‘1 min’，但流读 Hive 的 temporal join 的默认时间间隔是 ‘60 min’，这是因为当前流读 Hive 的 temporal join 实现上有一个框架限制，即每个 TM 都要访问 Hive metastore，这可能会对 metastore 产生压力，这个问题将在未来得到改善。 streaming-source.partition-order partition-name String streaming source 分区排序，支持 create-time， partition-time 和 partition-name。 create-time 比较分区/文件创建时间， 这不是 Hive metastore 中创建分区的时间，而是文件夹/文件在文件系统的修改时间，如果分区文件夹以某种方式更新，比如添加在文件夹里新增了一个文件，它会影响到数据的使用。partition-time 从分区名称中抽取时间进行比较。partition-name 会比较分区名称的字典顺序。对于非分区的表，总是会比较 ‘create-time’。对于分区表默认值是 ‘partition-name’。该选项与已经弃用的 ‘streaming-source.consume-order’ 的选项相同 streaming-source.consume-start-offset None String 流模式起始消费偏移量。如何解析和比较偏移量取决于你指定的顺序。对于 create-time 和 partition-time，会比较时间戳 (yyyy-[m]m-[d]d [hh:mm:ss])。对于 partition-time，将使用分区时间提取器从分区名字中提取的时间。 对于 partition-name，是字符串类型的分区名称(比如 pt_year=2020/pt_mon=10/pt_day=01)。 Flink 读取 Hive 原理 HiveSource&lt;T&gt;继承了AbstractFileSource&lt;T, HiveSourceSplit&gt;类，AbstractFileSource又实现了核心的Source接口。Hive表中是不存储数据的，所有数据存储在HDFS中，所以HiveSource继承了AbstractFileSource抽象类。通过FLIP-27我们可以了解到Source类的作用： 一个工厂类 用来创建SplitEnumerator和SourceReader 创建用来生成CheckPoint的序列化类，以及从状态中恢复Enumerator HiveSourceBuilderorg.apache.flink.connectors.hive.HiveSourceBuilder类主要是通过配置信息构建HiveSource类。HiveSourceBuilder类中核心的方法：buildWithBulkFormat。 HiveSourceHiveSource构造方法中的参数： Path[] inputPaths: 文件系统中的一个目录或者一个文件，buildHiveSource传过来的参数是：new Path[1] FileEnumerator.Provider fileEnumerator: 一个可以创建HiveSourceFileEnumerator的工厂类 FileSplitAssigner.Provider splitAssigner: 创建FileSplitAssigner工厂类 BulkFormat&lt;T, HiveSourceSplit&gt; readerFormat: 首先了解什么是BulkFormat：BulkFormat是一个接口，BulkFormat 一次读取并解析一批记录。BulkFormat的实现包括ORC Format、Parquet Format、HiveInputFormat等。 外部的BulkFormat类主要充当reader的配置持有者和工厂角色(用来创建reader的工厂)。BulkFormat.Reader是在BulkFormat#createReader(Configuration, FileSourceSplit)方法中创建的，然后完成读取操作。如果在流的checkpoint执行期间基于checkpoint创建Bulk reader，那么reader是在BulkFormat#restoreReader(Configuration, FileSourceSplit)方法中重新创建的。 ContinuousEnumerationSettings continuousEnumerationSettings: 流式读取持续监控分区和文件的配置，包括监控的时间间隔。 如果是批量读取continuousEnumerationSettings为空，如果是流式读取continuousEnumerationSettings会被new出来。 int threadNum: 用来限制最大创建监控Hive分区和文件的线程数，在org.apache.flink.connectors.hive.MRSplitsGetter类中Executors.newFixedThreadPool(threadNum);限制线程池的数量。 从table.exec.hive.load-partition-splits.thread-num中获取的参数。 JobConf jobConf: 通过HiveConf转成的JobConf ObjectPath tablePath:table/view/function的名称 List&lt;String&gt; partitionKeys: 分区键 ContinuousPartitionFetcher&lt;Partition, ?&gt; fetcher: 是一个Hive分区获取器，可以根据previousOffset获取之后的分区。需要利用HiveContinuousPartitionFetcherContext-getComparablePartitionValueList获取所有可比较的分区列表。 HiveTableSource.HiveContinuousPartitionFetcherContext&lt;?&gt; fetcherContext: 从Hive的Meta Store中获取表的分区的上下文。HiveSource继承AbstractFileSource类，AbstractFileSource-createReader会创建一个核心类FileSourceReader，用来读取分布式文件系统中的文件。HiveSource-createEnumerator会创建核心类ContinuousHiveSplitEnumerator(流式读取)或调用父类createEnumerator方法创建StaticFileSplitEnumerator(批方式读取)类。 ContinuousHiveSplitEnumerator分区的发现分为初始化阶段和后续持续监控阶段，ContinuousHiveSplitEnumerator的作用是定时发现分布式文件系统中的分区。ContinuousHiveSplitEnumerator实现了SplitEnumerator类，程序会定时调用该类的start()方法用来监控分区。start()方法中enumeratorContext.callAsync(monitor, this::handleNewSplits, discoveryInterval, discoveryInterval)方法的入参： monitor：回调类，会定期调用该类的call()方法 this::handleNewSplits：传入的是一个函数。用来处理monitor发现的新分区 discoveryInterval：初始化延迟时间 discoveryInterval：周期延迟时间 PartitionMonitor#call()方法call()方法会获取表的所有分区，并过滤掉currentReadOffset位点之后的之前的旧分区。然后根据这些分区循环生成HiveSourceSplit，最终返回NewSplitsAndState。NewSplitsAndState中存储三个全局变量： T offset: 最新的offset Collection&lt;List&gt; seenPartitions: 已经处理过的分区offset Collection newSplits: 监控到的新分区，如果没有监控到则为空 StaticFileSplitEnumeratorFileSource批处理SplitEnumerator的具体实现。获取文件系统目录中所有文件并分配给Reader。HiveSource的批处理使用该类做处理。 FileSourceReader12345678910public FileSourceReader( SourceReaderContext readerContext, BulkFormat&lt;T, SplitT&gt; readerFormat, Configuration config) { super( () -&gt; new FileSourceSplitReader&lt;&gt;(config, readerFormat), new FileSourceRecordEmitter&lt;&gt;(), config, readerContext); } 核心作用在构造方法中向父类传入() -&gt; new FileSourceSplitReader&lt;&gt;(config, readerFormat)。真正调用读取逻辑的是FileSourceSplitReader FileSourceSplitReaderFileSourceSplitReader类中fetch方法调用批量读取方法： 123456789@Overridepublic RecordsWithSplitIds&lt;RecordAndPosition&lt;T&gt;&gt; fetch() throws IOException { checkSplitOrStartNext(); final BulkFormat.RecordIterator&lt;T&gt; nextBatch = currentReader.readBatch(); return nextBatch == null ? finishSplit() : FileRecords.forRecords(currentSplitId, nextBatch);} BulkFormat在HiveSource构造方法参数中有详细讲解过。","link":"/posts/76537a8.html"},{"title":"Flink如何适配Hive中不同的数据存储格式","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) Hive支持的数据存储格式 TEXT 格式 (行式存储)TEXT FILE 是 Hive 默认文件存储方式，存储方式为行存储，数据不做压缩，磁盘占用大，数据解析使用资源多，数据不支持分片。 ORC格式 (列式存储)ORC文件格式可以提供一种高效的方法来存储Hive数据，运用ORC可以提高Hive的读、写以及处理数据的性能。 Parquet格式 (列式存储)Parquet 是面向分析型业务的列式存储格式，是一个面向列的二进制文件格式，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。Parquet对于大型查询的类型是高效的。对于扫描特定表格中的特定列的查询，Parquet特别有用。Parquet一般使用Snappy、Gzip压缩，默认是Snappy。 适配代码创建HiveSource的HiveSourceBuilder构造类中有两个build方法：buildWithDefaultBulkFormat()和buildWithBulkFormat(BulkFormat&lt;T, HiveSourceSplit&gt; bulkFormat)buildWithDefaultBulkFormat()使用默认BulkFormat处理数据读取，后者可以设置自定义读取格式。","link":"/posts/805c09f3.html"},{"title":"Flink Hive 合并小文件原理","text":"服务版本 Flink 1.5.2 MySQL 5.7.34 Flink CDC 2.2.1 Hive 3.0.0 同步场景我们的订单系统数据存储在事务型数据库MySQL中，订单系统中有一个核心的表order表。我们需要将order表中的数据实时同步到Hive的数据仓库中。Hive中的order表是根据订单时间的分区表。order表中order_date字段是下单时间，下单时间业务方设计是数据插入时间，由数据库now()函数生成。由于业务系统会有删除数据的情况，所以在Hive中需要能够有一个字段标识数据的状态。 同步配置Flink 集群安装Flink和Hive配置见文章：Flink和Hive配置 Flink CDC 依赖安装本次测试我们使用SQL Client进行SQL任务的提交。下载 flink-sql-connector-mysql-cdc-2.3-SNAPSHOT.jar 到 /lib/ 目录下。下载 mysql-connector-java-8.0.28.jar 到 /lib/ 目录下。 注意：如果您已经启动Flink集群，添加上述依赖后需要对集群进行重启。 MySQL配置MySQL需要开启BinLog日志。安装见：Docker安装MySQL建库建表： 1234567891011121314151617181920212223242526272829SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;CREATE DATABASE `db_order`;USE db_order;-- ------------------------------ Table structure for t_order-- ----------------------------DROP TABLE IF EXISTS `t_order`;CREATE TABLE `t_order` ( `order_id` int(10) NOT NULL AUTO_INCREMENT, `order_date` datetime NOT NULL, `customer_name` varchar(100) DEFAULT NULL, `price` decimal(10,5) DEFAULT NULL, `product_id` int(10) DEFAULT NULL, `order_status` bit(1) DEFAULT NULL, PRIMARY KEY (`order_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;INSERT INTO `db_order`.`t_order` (`order_id`, `order_date`, `customer_name`, `price`, `product_id`, `order_status`) VALUES (1, '2022-10-12 15:14:36', '客户1', 129.14200, 10001, b'0');INSERT INTO `db_order`.`t_order` (`order_id`, `order_date`, `customer_name`, `price`, `product_id`, `order_status`) VALUES (2, '2022-10-12 15:14:36', '客户2', 130.14200, 10002, b'0');INSERT INTO `db_order`.`t_order` (`order_id`, `order_date`, `customer_name`, `price`, `product_id`, `order_status`) VALUES (3, '2022-10-12 15:14:36', '客户3', 130.14200, 10003, b'0');INSERT INTO `db_order`.`t_order` (`order_id`, `order_date`, `customer_name`, `price`, `product_id`, `order_status`) VALUES (5, '2022-10-13 15:14:36', '客户5', 130.14200, 10003, b'0');DELETE FROM `db_order`.`t_order` where order_id = 5;UPDATE `db_order`.`t_order` SET `order_date` = '2022-10-12 15:14:36', `customer_name` = '客户22', `price` = 130.14200, `product_id` = 10002, `order_status` = b'0' WHERE `order_id` = 2;SET FOREIGN_KEY_CHECKS = 1; Hive 配置Hive中创建分区表，order_date订单时间，使用订单时间的年、月、日作为三个分区字段。表的存储格式使用ORC格式。使用Hive的sql-client执行建表语句，或者使用Flink的sql-client，如果使用flink sql-client，需要切换成Hive方言SET 'table.sql-dialect' = 'hive'; NOTE: 为了使用 Hive 方言, 你必须首先添加和 Hive 相关的依赖. 请参考 Hive dependencies 如何添加这些依赖。 从 Flink 1.15版本开始，如果需要使用 Hive 方言的话，请首先将 FLINK_HOME/opt 下面的 flink-table-planner_2.12 jar 包放到 FLINK_HOME/lib 下，并将 FLINK_HOME/lib 下的 flink-table-planner-loader jar 包移出 FLINK_HOME/lib 目录。否则将抛出 ValidationException。具体原因请参考 FLINK-25128。 请确保当前的 Catalog 是 HiveCatalog. 否则, 将使用 Flink 的默认方言。 在启动了 HiveServer2 endpoint 的 SQL Gateway，默认当前的 Catalog 就是 HiveCatalog。 为了实现更好的语法和语义的兼容，强烈建议首先加载 HiveModule 并将其放在 Module 列表的首位，以便在函数解析时优先使用 Hive 内置函数。 请参考文档 here 来将 HiveModule 放在 Module 列表的首。 在启动了 HiveServer2 endpoint 的 SQL Gateway，HiveModule 已经被加载进来了。 Hive 方言只支持 db.table 这种两级的标识符，不支持带有 Catalog 名字的标识符。 虽然所有 Hive 版本支持相同的语法，但是一些特定的功能是否可用仍取决于你使用的 Hive 版本。例如，更新数据库位置 只在 Hive-2.4.0 或更高版本支持。 Hive 方言主要是在批模式下使用的，某些 Hive 的语法(Sort/Cluster/Distributed BY, Transform, 等)还没有在流模式下支持。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117CREATE TABLE IF NOT EXISTS order_orc_no_policy( order_id int, order_date Timestamp, customer_name string, price decimal(10, 5), product_id int, order_status BOOLEAN, op string, `kafka_timestamp` TIMESTAMP) COMMENT 'order_orc' PARTITIONED BY( `order_date_dt` string, `order_date_hr` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES ( 'sink.partition-commit.policy.kind'='metastore,success-file' );INSERT INTO myhive.yyq.order_orc_no_policy select order_id, order_date, customer_name, price, product_id, order_status, type, `timestamp`, DATE_FORMAT(order_date, 'yyyy-MM-dd'), DATE_FORMAT(order_date, 'HH') from default_catalog.default_database.k_order_read;CREATE TABLE IF NOT EXISTS order_orc_auto_compaction( order_id int, order_date Timestamp, customer_name string, price decimal(10, 5), product_id int, order_status BOOLEAN, op string, `kafka_timestamp` TIMESTAMP ) COMMENT 'order_orc' PARTITIONED BY( `order_date_dt` string, `order_date_hr` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( 'auto-compaction'='true', 'compaction.file-size'='128MB', 'sink.partition-commit.policy.kind'='metastore,success-file');INSERT INTO myhive.yyq.order_orc_auto_compaction select order_id, order_date, customer_name, price, product_id, order_status, type, `timestamp`, DATE_FORMAT(order_date, 'yyyy-MM-dd'), DATE_FORMAT(order_date, 'HH') from default_catalog.default_database.k_order_read;CREATE TABLE IF NOT EXISTS order_orc_partition_commit( order_id int, order_date Timestamp, customer_name string, price decimal(10, 5), product_id int, order_status BOOLEAN, op string, `kafka_timestamp` TIMESTAMP ) COMMENT 'order_orc' PARTITIONED BY( `order_date_dt` string, `order_date_hr` string, `order_date_min` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( 'sink.partition-commit.trigger'='process-time', 'sink.partition-commit.delay'='0s', 'sink.partition-commit.watermark-time-zone'='Asia/Shanghai', 'sink.partition-commit.policy.kind'='metastore,success-file', 'sink.parallelism'='5');INSERT INTO myhive.yyq.order_orc_partition_commit select order_id, order_date, customer_name, price, product_id, order_status, type, `timestamp`, DATE_FORMAT(order_date, 'yyyy-MM-dd'), DATE_FORMAT(order_date, 'HH'), DATE_FORMAT(order_date, 'mm') from default_catalog.default_database.k_order_read;CREATE TABLE IF NOT EXISTS order_orc_partition_commit_and_auto_compaction( order_id int, order_date Timestamp, customer_name string, price decimal(10, 5), product_id int, order_status BOOLEAN, op string, `kafka_timestamp` TIMESTAMP ) COMMENT 'order_orc' PARTITIONED BY( `order_date_dt` string, `order_date_hr` string, `order_date_min` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( 'sink.partition-commit.trigger'='process-time', 'sink.partition-commit.delay'='0s', 'sink.partition-commit.watermark-time-zone'='Asia/Shanghai', 'sink.partition-commit.policy.kind'='metastore,success-file', 'sink.parallelism'='3', 'auto-compaction'='true', 'compaction.file-size'='128MB');INSERT INTO myhive.yyq.order_orc_partition_commit_and_auto_compaction select order_id, order_date, customer_name, price, product_id, order_status, type, `timestamp`, DATE_FORMAT(order_date, 'yyyy-MM-dd'), DATE_FORMAT(order_date, 'HH'), DATE_FORMAT(order_date, 'mm') from default_catalog.default_database.k_order_read; 开始同步 同步之前请确保SQL Client已经注册Hive CATALOG，Flink CDC可以正常访问MySQL。 123456789101112131415161718192021222324-- 设置checkpoint间隔时间为10s，生产环境建议2-3分钟 SET 'execution.checkpointing.interval' = '30s'; -- 创建连接MySQL BinLog的Flink表CREATE TABLE f_order ( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, PRIMARY KEY(order_id) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'hostname' = '192.168.80.92', 'port' = '3308', 'username' = 'root', 'password' = '123456', 'database-name' = 'db_order', 'table-name' = 't_order');select * from f_order;SET 'execution.savepoint.path' = 'hdfs://dn8092:8020/flink/checkpoints/fd2b980717b5ca23efb1be219bdd8c14/chk-3'; 测试Flink CDC能不能正常获取MySQL的BinLog。 12select * from f_order;-- 按 q 退出查询 如果能够正常获取则显示下图，最新数据的获取刷新是根据SET 'execution.checkpointing.interval' = '10s';的配置间隔显示。 将数据MySQL BinLog数据使用canal格式写入Kafka中 12345678910111213141516171819202122232425-- 创建Kafka连接表CREATE TABLE k_order_write_canal( order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN, op STRING METADATA FROM 'op' VIRTUAL, PRIMARY KEY(order_id) NOT ENFORCED) WITH ( 'connector' = 'kafka', 'topic' = 'flink-cdc-kafka-order', 'properties.bootstrap.servers' = 'dn90222:6667,dn90224:6667,dn90225:6667,dn90226:6667', 'properties.group.id' = 'flink-cdc-kafka-group', 'value.format' = 'canal-json', 'properties.security.protocol' = 'SASL_PLAINTEXT', 'properties.sasl.mechanism' = 'GSSAPI', 'properties.sasl.kerberos.service.name' = 'kafka', 'properties.allow.auto.create.topics' = 'true' );-- 将 MySQL BinLog数据写入 Kafka中insert into k_order_write_canal select * from f_order; 将数据从Kafka中同步到Hive 123456789101112131415161718192021222324252627282930313233-- 创建Kafka读表CREATE TABLE k_order_read( data ARRAY&lt;ROW&lt; order_id INT, order_date TIMESTAMP(0), customer_name STRING, price DECIMAL(10, 5), product_id INT, order_status BOOLEAN&gt;&gt;, order_id as data[1].order_id, order_date as data[1].order_date, customer_name as data[1].customer_name, price as data[1].price, product_id as data[1].product_id, order_status as data[1].order_status, `type` STRING, `timestamp` TIMESTAMP(3) METADATA FROM 'timestamp', WATERMARK FOR order_date AS order_date - INTERVAL '5' SECOND) WITH ( 'connector' = 'kafka', 'topic' = 'flink-cdc-kafka-order', 'properties.bootstrap.servers' = 'dn90222:6667,dn90224:6667,dn90225:6667,dn90226:6667', 'properties.group.id' = 'flink-cdc-kafka-group', 'value.format' = 'json', 'properties.security.protocol' = 'SASL_PLAINTEXT', 'properties.sasl.mechanism' = 'GSSAPI', 'properties.sasl.kerberos.service.name' = 'kafka', 'properties.allow.auto.create.topics' = 'true' );-- 同步前请确保Hive的Catalog已经注册INSERT INTO myhive.yyq.order_orc select order_id, order_date, customer_name, price, product_id, order_status, type, `timestamp`, DATE_FORMAT(order_date, 'yyyy-MM-dd'), DATE_FORMAT(order_date, 'HH') from default_catalog.default_database.k_order_read; 在Hive中是历史拉链表，所有的增删改都有记录，通过下面SQL可以过滤掉删除的、旧的数据。 12select * from (select order_id, max(kafka_timestamp) as tp from order_orc group by order_id) t join order_orc oc on t.order_id = oc.order_id and t.tp = oc.kafka_timestamp where oc.op != 'DELETE'; 配置解析我们在创建Hive表的时候增加了几个特殊的配置： 12345'partition.time-extractor.timestamp-pattern'='$order_date_dt $order_date_hr:00:00','sink.partition-commit.trigger'='partition-time','sink.partition-commit.delay'='3 m','sink.partition-commit.watermark-time-zone'='Asia/Shanghai','sink.partition-commit.policy.kind'='metastore,success-file' partition.time-extractor.timestamp-pattern sink.partition-commit.trigger sink.partition-commit.delay sink.partition-commit.watermark-time-zone sink.partition-commit.policy.kind","link":"/posts/573ecee0.html"},{"title":"Flink Formats 使用以及原理","text":"Flink 版本1.15.2 案例： 123456789CREATE TABLE source_table( column_name1 INT, column_name2 STRING) WITH ( 'connector' = 'filesystem', 'path' = 'file:///Users/wudongming/Documents/datatom/code/flink/flink-examples/flink-examples-table/src/test/resources/test.json', 'format' = 'json' ) 配置中 'format' = 'json'，format的配置不仅可以是JSON，还可以是CSV，等等其他配置：链接 如何实现不同根据配置加载不同format：样例代码 123456789101112131415161718192021222324252627282930313233package org.apache.flink.table.examples.java.file;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;public class FileDynamicTableTestCase { public static void main(String[] args) { Configuration configuration = new Configuration(); StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment( configuration); StreamTableEnvironment envT = StreamTableEnvironment.create(env); env.setParallelism(1); String sourceSQL = &quot;CREATE TABLE source_table (\\n&quot; + &quot; column_name1 INT,\\n&quot; + &quot; column_name2 STRING\\n&quot; + &quot;) WITH (\\n&quot; + &quot; 'connector' = 'filesystem',\\n&quot; + &quot; 'path' = 'file:///Users/wudongming/Documents/datatom/code/flink/flink-examples/flink-examples-table/src/test/resources/test.json',\\n&quot; + &quot; 'format' = 'json'\\n&quot; + &quot;)&quot;; String printSQL = &quot;CREATE TABLE print_table WITH ('connector' = 'print')\\n&quot; + &quot;LIKE source_table (EXCLUDING ALL)&quot;; String etlSQL = &quot;insert into print_table select * from source_table&quot;; envT.executeSql(sourceSQL); envT.executeSql(printSQL); envT.executeSql(etlSQL); }} INSERT INTO 之后根据Flink Table的逻辑会找到Source表和Sink表，程序会根据Source表中'connector' = 'filesystem'配置加载org.apache.flink.connector.file.table.FileSystemTableFactory类FileSystemTableFactory文件链接器的工厂类，我们这段逻辑文件连接器是Source，所以会调用FileSystemTableFactory#createDynamicTableSource。createDynamicTableSource方法会返回构建好的org.apache.flink.connector.file.table.FileSystemTableSource类，FileSystemTableSource构造方法中有两个重要的参数bulkReaderFormat和deserializationFormatbulkReaderFormat：是批量读取文件时的格式化类，通常用于ORC、AVRO、CSV、Parquet文件的格式化。deserializationFormat用于按行读取的格式化类，通常用于JSON、CSV、AVRO、Maxwell、PggJson等等。createDynamicTableSource 构建 FileSystemTableSource完成之后，Flink Table会调用FileSystemTableSource类的getScanRuntimeProvider方法来初始化扫描数据的类。getScanRuntimeProvider方法会先处理分区、元数据、处理字段投影，然后再处理bulkReaderFormat和deserializationFormat先了解 DecodingFormat、ProjectableDecodingFormat、BulkDecodingFormat接口的关系。flink-table/flink-table-common模块下org.apache.flink.table.connector.format.Format是所有格式化接口的父类，最底层的连接器数据格式化接口。也是DecodingFormat和EncodingFormat接口的父类。DecodingFormat是负责读取格式化的接口，该接口中抽象方法createRuntimeDecoder(DynamicTableSource.Context context, DataType physicalDataType)，physicalDataType建表时的字段以及字段类型。这个 DataType 通常是从表的 ResolvedSchema派生出来的，并且不包括分区、元数据和其他辅助列。physicalDataType应该准确描述完整的序列化记录。换句话说：对于序列化记录中的每个字段，在 physicalDataType的相同位置都有一个相应的字段。有些实现可能允许用户省略字段，但这取决于数据格式的特点。例如，CSV格式化的实现可能允许用户为每行10列的前5列定义结构。如果格式支持推测（projections）：可以排除一些不字段，让这些字段不被解析，并且可以在生成的RowData中重新排序字段，那么他应该实现ProjectableDecodingFormat接口。比如DebeziumJsonDecodingFormat实现类中，Debezium产生的数据格式是原始的DebeziumJson数据字段很多，有可能部分字段我们用不到，DebeziumJsonDecodingFormat实现的是ProjectableDecodingFormat接口，则在读取的时候就可以排除一部分字段。ProjectableDecodingFormat#createRuntimeDecoder相比较父类的createRuntimeDecoder多了int[][] projections参数，传入的是需要的字段，其他字段非必须字段则不会被解析。","link":"/posts/3ae81791.html"},{"title":"Flink Projection 使用以及原理","text":"建表语句： 1234567CREATE TABLE t( i INT, r ROW &lt; d DOUBLE, b BOOLEAN&gt;, s STRING); 查询语句 12SELECT s, r.dFROM t; 上面的例子中建表字段是","link":"/posts/a4786e67.html"},{"title":"什么是 Java","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) 维基百科中的Java 维基百科中的Java Java是一个高层次的(high-leve)、以类为基础的(class-based)、面向对象的(object-oriented)编程语言。Java的宗旨是一次编写，随处运行。也就是编译后的Java代码可以在所有支持Java的平台上运行，无需再重新编译。 Java 官网中的什么是Java https://www.java.com/download/help/whatis_java.html Java 发展历史","link":"/posts/9159529f.html"},{"title":"Java 安装教程","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) 下载JDK下载地址 下载最新版本JDK不用登录账号，但是下载JDK1.8需要登录账号，如果不想注册，可以使用我提供的账号（贴心不 :smirk: ）。 121414807686@qq.comCodedm96@codedm.com 下载页面往下拉可以看见Java8，根据您不同系统下载不同版本的JDK。 Windows安装JDK 双击打开安装包 点击下一步 点击下一步 点击下一步 设置中搜索 环境变量 -&gt; 点击编辑系统环境变量 -&gt; 点击环境变量 点击新建 新建Java Home 默认安装路径：C:\\Program Files\\Java\\ 下面的变量值建议到这个文件夹中复制，小版本号可能会存在差异。 变量名：JAVA_HOME变量值：C:\\Program Files\\Java\\jdk1.8.0_341 新建CLASSPATH 变量名：CLASSPATH变量值：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; 修改Path(注意是修改) 修改界面点击新建 新建两个： 12%JAVA_HOME%\\bin%JAVA_HOME%\\jre\\bin 所有窗口点击确定 MacOS安装JDK 下载完成之后双击打开，再双击黄色盒子 点击继续 -&gt; 点击安装 -&gt; 点击关闭 配置环境变量 打开终端输入下面命令 vim .zshrc 按下 i 键，再粘贴输入(注意修改成你自己的路径)： 1234567891011121314JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_341.jdk/Contents/HomePATH=$JAVA_HOME/bin:$PATH:.CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.export JAVA_HOMEexport PATHexport CLASSPATH````&gt; 注意修改成你自己的版本号jdk1.8.0_341.jdk按键盘 esc -&gt; 输入 :wq -&gt; 回车再输入命令 source .zshrc 12345678910 &gt; 默认安装路径为： /Library/Java/JavaVirtualMachines/jdk1.8.0_341.jdk/Contents/Home## Linux安装JDK## 测试安装命令行输入： java -version 123显示出版本号则证明安装成功。 java version “1.8.0_341”Java(TM) SE Runtime Environment (build 1.8.0_341-b10)Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode) 123命令行输入java以及javac均不报错： java 123以及 javac ```","link":"/posts/823e9fb.html"},{"title":"第一个 Java 程序 - Hello World","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我)前两篇文章我们了解了什么是Java以及Java在各个平台的安装过程，这篇起，我们开始快乐的写Java代码，做一个Java程序员。 新建class文件首先随便找一个地方新建一个HelloWorld.java文件，然后用记事本打开他，Mac环境可以使用文本编辑器。然后一个字母一个字母打出下面的代码(不准复制。。)： 123456public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }} 命令行式运行在当前文件夹下打开命令行工具，Windows 打开CMD，Mac打开终端。输入命令 1javac HelloWorld.java 没有报错，没有出现异常。文件夹中出现HelloWorld.class文件，这是javac编译后的可运行文件。执行class文件： 1java HelloWorld 控制台会打印出HelloWorld，程序运行成功。 Idea开发工具运行下载安装Idea（略过，自己百度去），我还是给你们留了一个教程连接：安装教程安装教程创建完一个project，在project的src/main/java下创建一个类，命名HelloWorld。输入代码点击绿色箭头运行，下方控制台会出现我们程序要求打印的Hello World 代码讲解 注释常用注释两种 1234567单行注释 // 多行注释/** * 我的第一个Java程序 * * @author Code-dm */ 类（class）类名必须和文件名一样。暂时先理解为一个代码容器，后续的面向对象会详细讲解。 Main方法public static void main(String[] args)程序的主入口，一个程序只能有一个主入口。且每个程序都必须包含一个 main() 方法。 打印System.out.println()可以将字符串打印到控制台中。 System是Java核心库中的一个类 out是控制输出的对象 println()接收一个字符串并通过write输出到控制台","link":"/posts/6913f6d4.html"},{"title":"Flink CheckPoint 使用以及原理","text":"CheckPoint 使用场景比如我们现在使用Flink CDC来获取MySQL的变更日志并实时计算订单总金额。Flink程序时24小时不停止的运行的，运行了几天之后程序出错了。如果没有CheckPoint程序是不是需要从几天前开始运行。如果你开启了CheckPoint只需要从上一次成功的CheckPoint位置开始执行。CheckPoint设计的作用是为了Flink程序容错。 什么是CheckPointFlink基于Chandy-Lamport算法（分布式快照算法，相关论文）技术提供了CheckPoint容错机制，分布式快照可以将同一时间点Task/Operate的状态数据做全局统一快照处理。这个状态数据可以存储在内存、HDFS、S3等等","link":"/posts/fed98383.html"},{"title":"填写标题","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我)","link":"/posts/516f9a96.html"},{"title":"Java中的面向对象OOP","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我)面向对象OOP（Object-oriented programming） 面向过程 Process-oriented programming, POP开个网店本章节很重要，OOP是Java编程的核心思想，所以本章节我会尽力写的更加详细，用更多的例子来讲解。在讲Java的面向对象之前我们想讲讲为什么会有面向对象这个东西。在面向对象编程思想之前有一种面向过程(Process-oriented programming, POP)的编程思想，C语言其实就是面向过程的，面向过程的意思是我们按照事件发展的流程来写代码，每行代码对应事件发展的每个节点，直到事件结束，代码也就结束了。伴随着软件的发展，需求越来越复杂，代码也来越庞大，事件发展的分支越来越多，面向过程的弊端就凸显了，因为根本就没办法维护，修改一个小需求需要把整个过程走一遍。我们理解OOP的最好办法就是来看看面向过程在复杂场景为什么会这么吃力。文中使用案例来自：https://dins.site/coding-advanced-oop-motivation-chs/假设你是一个水果商人，你看到了互联网的发展给零售业带来的冲击和商机，于是你准备自己办一个网站，线上卖水果。由于你自己懂编程，所以你想自己写一个后台，把web端交给另一个人去做。这样即节省成本又有乐趣。于是你们两个约定好，web端给用户提供各种选择，其结果用二维数组的方式传给后台，后台根据文件计算水果价格收据，再返回给web端。这个故事发生在90年代，所以不需要考虑其他复杂的因素，把程序做出来就可以了。为了方便我们使用控制台的方式，通过控制台代替文本来传输各种选择给后端，后端计算价格在输出给控制台。 1234567891011121314151617181920212223242526272829303132333435/** * 面向过过程的程序设计 * * @author Codedm */public class PopDemo { public static void main(String[] args) { // 定义一个二维数组 存储水果序号、水果名称、水果单价 String[][] fruitData = { {&quot;苹果&quot;, &quot;2&quot;}, {&quot;橘子&quot;, &quot;5&quot;}, {&quot;香蕉&quot;, &quot;10&quot;}, {&quot;芒果&quot;, &quot;13&quot;}, {&quot;葡萄&quot;, &quot;15&quot;} }; System.out.println(&quot;请输入您需要购买水果的序号：&quot;); // 使用for循环打印各个水果名称和序号 for (int i = 0; i &lt; fruitData.length; i++) { System.out.println(&quot;序号：&quot; + i + &quot;，&quot; + fruitData[i][0]); } Scanner sc = new Scanner(System.in); int fruitNumber = sc.nextInt(); System.out.println(&quot;您选择的水果是：&quot; + fruitData[fruitNumber][0] + &quot;，它的单价是：&quot; + fruitData[fruitNumber][1]); System.out.println(&quot;请输入购买的重量：&quot;); // 此场景仅仅为了演示面向过程的编码风格，请忽略使用int类型来接收重量和价格。 int weight = sc.nextInt(); int totalPrice = Integer.parseInt(fruitData[fruitNumber][1]) * weight; System.out.println(&quot;总价为：&quot; + totalPrice); }} 吸引更多的顾客大家思考下现在我们是简单的计算价格的逻辑，渐渐的有更多的人来在线购买，同时为了吸引更多的用户，我们想发放优惠券。优惠券的逻辑还要增加在这个现有的逻辑里面。再比如还会有满减，配送费等等其他复杂的逻辑。 FOP为什么会失败？上述代码可以看出POP是紧耦合的，需求变更必然需要修改原来的代码。这点如果在代码不复杂的请看下看不出来，但是随着代码量越来越多问题就会越来越大。企业级的代码肯定不是几十行或者几百行可以完成的。OOP的出现其实就是从设计上来解决这种复杂代码的变更带来的维护问题。 面向对象OOP什么是类和对象在面向对象的编程中，类是来用创建对象的图纸、蓝图。通常来说一个类可以表示为一个人、一个地方或者一个事物，是一种抽象。比如说盖一个房子，房子的图纸（这张纸）是Java中的.java 文件，图纸上房子的架构就是类，根据这个图纸盖出来的房子是对象。对象是类的一个具象化的实体。我们可以拿这个图纸盖很多个房子，所以类只有一个，一个类可以构造出多个对象。 在Java中创建一个类","link":"/posts/fd4a56f1.html"},{"title":"运算符和执行流程控制","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我) Java中常用的的基本运算符数学运算符+-*/% 数学运算符和数学中运算符一样，使用的符号也是一样用。 递增和递减运算符++ 和 -- ++表示当前这个变量的值增加 1，-- 则相反，减 1 ，开发中使用最多的是 i++ 和 ++i，i 表示一个变量的名称。 12345678// i++ 和 ++i 的区别int i = 1;int j = ++i;System.out.println(j); // 2int x = 1;int y = x++;System.out.println(y); // 1 上述代码定义的 i 初始值为 1 ，然后再对 i 进行 ++i ，把 ++i 的值赋给 j ， j 最终打印出来的值为 2。下面的代码和下面的代码同理，只是变换成了 x++，打印出来的却是 1。拆分代码来理解， ++i 其实相当于 i = i + 1 然后再把 i 的值赋给 j，所以 j 的值是 2。x++ 相当于先把x 的值赋给 y，然后再执行 x = x + 1，所以 y 的值是 1 。 比较运算符&gt;&lt;&gt;=&lt;=instanceof比较运算符会返回 true 和 false 123// 比较运算符boolean t = 2 &gt; 1;System.out.println(); // true 逻辑运算符 &amp;&amp; ||&amp;&amp;||!&amp;&amp; 表示前后必须同时满足 true ，才是真的 true 。 123// &amp;&amp;System.out.println(2 &gt; 1 &amp;&amp; 3 &gt; 1); // trueSystem.out.println(2 &gt; 1 &amp;&amp; 0 &gt; 1); // false &amp; 和 &amp;&amp; 区别，我找了两篇文章可以看看： https://stackoverflow.com/questions/7199666/difference-between-and-in-java https://www.tutorialspoint.com/Differences-between-and-and-and-and-operators-in-Java 两篇文章都提到 &amp; 是按位运算符，&amp;&amp; 是合乎逻辑的。&amp; 评估的是给定数字的二进制的值，当 &amp; 开始运算时，它会从左边开始计算两个数字中字符的值。来个例子： 12System.out.println(10 &amp; 12);// returns 8 10 的二进制的值是 1010，12 的的二进制的值是 1100再开始运算之前我们需要知道一个小知识点： 1234System.out.println(1 &amp; 0); // 0System.out.println(0 &amp; 1); // 0System.out.println(1 &amp; 1); // 1System.out.println(0 &amp; 0); // 0 再来看看 &amp;&amp; ，它是按照逻辑推算的，它的逻辑规则是所有的表达式都是 true 整体表达式才是 true，只要运行到一个节点是false后面的就不会执行了。这被又称作短路。 || 表示前后有一个是 true 则返回 true 123// ||System.out.println(2 &gt; 1 || 3 &gt; 1); // trueSystem.out.println(2 &gt; 1 || 0 &gt; 1); // true ! 表示取反，意思是如果是 true 加上 ! 就是 false。 三元运算符三元运算符的使用场景一般是使用一个判断条件从两个选择中选择一个。 使用方法 condition ？ 选择一 : 选择二 如果 condition 是 true 则返回 选择一 ，如果是 false 则返回 选择二。 1234// 三元运算符int z = 2 &gt; 1 ? 10 : 11;System.out.println(z); // 10 2 &gt; 1 是 true，所以返回 10，则 z 的值为 10。 流程控制if 判断流程图中有一个菱形的图形，表示一个判断逻辑，会分出两个分支 是 分支和 否 分支。 if 语句和这个菱形的作用差不多。都是用来判断。 123if (判断条件) { // 如果判断条件是true，则执行大括号里面的代码。} 12345if (判断条件) { // 如果判断条件是true，则执行大括号里面的代码。} else { // 如果是false则执行这个大括号里面的代码。} 1234567if (判断条件1) { // 如果判断条件1是true，则执行大括号里面的代码。} else if (判断条件2) { // 如果判断条件2是true，则执行该大括号里面的代码。} else { // 如果上述判断条件都不满足则执行这个else大括号。} 1234567891011121314151617public class Judgment { public static void main(String[] args) { // 接收控制台输入 Scanner scanner = new Scanner(System.in); String input = scanner.nextLine(); // equals方法可以比较两个字符串是否一样 if (&quot;A&quot;.equals(input)) { System.out.println(&quot;AA&quot;); } else if (&quot;B&quot;.equals(input)) { System.out.println(&quot;BB&quot;); } else { System.out.println(&quot;CC&quot;); } }} SWITCH和 if 以及 if-else 不同的是，switch 是可以有多条执行路径的。java8中 switch 可以用byte、short、char 和 int原始数据类型来做判断。也支持枚举类型、String类型还有一些包装类。来个例子，需求是把数字1-12转成中文的1月-12月： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @author Codedm */public class SwitchDemo { public static void main(String[] args) { int month = 8; String monthString; switch (month) { case 1: monthString = &quot;一月&quot;; break; case 2: monthString = &quot;二月&quot;; break; case 3: monthString = &quot;三月&quot;; break; case 4: monthString = &quot;四月&quot;; break; case 5: monthString = &quot;五月&quot;; break; case 6: monthString = &quot;六月&quot;; break; case 7: monthString = &quot;七月&quot;; break; case 8: monthString = &quot;八月&quot;; break; case 9: monthString = &quot;九月&quot;; break; case 10: monthString = &quot;十月&quot;; break; case 11: monthString = &quot;十一月&quot;; break; case 12: monthString = &quot;十二月&quot;; break; default: monthString = &quot;无法匹配！&quot;; break; } System.out.println(monthString); }} 执行之后将会打印八月。现在需求变了，我需要根据输入数字的1-12，输出对应的季节。 12345678910111213141516171819202122232425262728293031323334353637/** * @author Codedm */public class SwitchSeason { public static void main(String[] args) { int month = 8; String monthString; switch (month) { case 1: case 2: case 12: monthString = &quot;冬季&quot;; break; case 3: case 4: case 5: monthString = &quot;春节&quot;; break; case 7: case 8: case 6: monthString = &quot;夏季&quot;; break; case 10: case 11: case 9: monthString = &quot;秋季&quot;; break; default: monthString = &quot;无法匹配！&quot;; break; } System.out.println(monthString); }} 当前这个程序会打印夏季。swich中的 default 表示上述的所有条件都不能满足或没有进行break会执行 default代码块。break的作用表示 switch 代码块中该 break 下面的代码不用执行了。 switch 相较于 if 拥有更强的可读性，如果有大量的 if 语句时会推荐使用 switch 来进行重构。 for循环假如我们想重复的输出一个字符串10次，如何实现呢： 123System.out.println(&quot;Hello World&quot;);System.out.println(&quot;Hello World&quot;);。。。 10行输出，这是一种实现方式（这方式也太蠢了）。使用for来解决。 12345678public class ControlFlow { public static void main(String[] args) { for (int i = 0; i &lt; 3; i++) { System.out.println(&quot;Hello World&quot;); } }} for循环中做了哪些： for() 括号中定义了一个变量 i 定义了 i 的循环条件是小于3的 每次循环后i都会进行 +1 for循环的执行流程：蓝色背景那行代码执行完成之后我们就已经初始化了 i 并给 i 赋值为 0，同时判断 0 是小于 3 的，符合循环条件，执行循环体（for() {}，{}大括号内的代码称为循环体），再执行 i++，由文章上面已经讲解了 i++ 是先进行赋值再进行自加 1，此时 i 变为 1。再跳到 for所在的那行，进行判断 i 值为 1 是小于 3 的继续执行循环体。直到 i 不再小于 3 就会执行 for 循环体后的代码。 for-eachwhile 和 do whilebreak 和 continue的区别","link":"/posts/2a65962b.html"},{"title":"Java 的数据类型","text":"本篇文章对应的Issues，Issues代表我文章创作的过程。(点击我)，Issues代表我文章创作的过程。(点击我) 什么是变量变量的定义我们由一个停车场示例来讲解什么是变量。在C镇有很多停车场，小区停车场和公共停车场，停车场内根据不同大小的车辆划分了不同的停车位（大中小）。车辆停放必须根据车子的尺寸停放，先定义停车场的规则： 小区内的停车场只能停放该小区的车辆 小区外的公共停车场允许多个小区停放 车辆必须根据车子的尺寸严格停放到对应的车位中 将示例中各个角色和我们今天的变量对应停车场 -&gt; 变量 汽车 -&gt; 变量中的值公共停车场 —&gt; 成员变量小区内停车场 -&gt; 局部变量不同尺寸停车位 -&gt; Java中数据类型根据代码再来看看什么是变量： 1234567891011public class Variable { public static void main(String[] args) { int a; // 定义一个int类型的变量 a int x, y; // 定义两个int类型的变量 x 和 y int c = 1, d = 2; // 定义变量 c 赋值 1 和变量 d 赋值 2 a = 0; // 给 a 赋值 0 a = 1; // 把 a 的值变为 1 }} 从上面代码中可以看出定义一个变量首先要确定变量的类型，再给变量起个好听的名字，最后给变量赋值。变量为什么叫 “变”量，是因为他是可以改变的，也就是可以重新赋值。看例子。 12345678910public class Variable { public static void main(String[] args) { int i = 10; // 定义一个 int 类型变量并赋值为 10 System.out.println(i); // 打印 这个变量 输出 10 i = 11; // 重新赋值 11 System.out.println(i); // 打印 这个变量 输出 11 i 是可以重新赋值的 }} 变量的作用范围成员变量和局部变量最大的区别是作用范围不一样 1234567891011121314public class Variable { public static void main(String[] args) { int i = 10; if (true) { int b = 20; System.out.println(num); System.out.println(i); System.out.println(b); } // System.out.println(b); 编译报错 b 不在这个括号的作用范围内 }} 上述代码中 i 的作用范围在他所在大括号内b 的作用范围在他所在大括号内 i 的作用范围要比 b 的作用范围更广。 Java中基本数据类型 对应代码 -&gt; com.codedm.java.basic.datatype.PrimitiveDataType Java中有8种基本数据类型：byte存储从 -128 到 127 的整数 占用字节数： 1 byte 1byte b = 100; short存储从 -32,768 到 32,767 的整数 占用字节数： 2 byte 1short s = 100; int存储从 -2,147,483,648 到 2,147,483,647 的整数 占用字节数： 4 byte 1int t = 100; long存储从 -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807 的整数 占用字节数： 8 byte 1long l = 100; float存储一个分数，能够储存6至7位的小数 占用字节数： 4 byte 1float f = 100.1f; double存储一个分数，能够储存15位的小数 占用字节数： 8 byte 1double d = 100.1; boolean存储一个 true 和 false 占用字节数： 1 bit 1boolean bl = true; char存储单个字符 占用字节数： 2 byte 1char c = 'C'; 从byte -> short -> int -> long 虽然存储的都是整数但是能够存储的位数是不一样的，位数越小占用的空间也就越小，选择合适的变量能够优化程序运行时占用的内存空间。 我们假设 ``1 byte`` 代表一个空格。 ![占用存储空间](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20220730/646efe8dcaa546f9a339774f1035ab66.png?x-oss-process=style/codedm) 虽然 ``int``也是存储 ``1`` 这个值，但是他占用了4个格子，即使格子是空的也会占用内存空间。 Java 中的引用数据类型除了8大基本数据类型其他的全部是引用数据类型 StringString 类型用来存储一段文本。(该类后面会详细讲解) 1String name = &quot;Codedm&quot;; 数组多个相同的数据类型组成的集合。 数学中集合的概念是：集合是指具有某种特定性质的具体的或抽象的对象汇总而成的集体。其中，构成集合的这些对象则称为该集合的元素。 数组和集合的概念类似：多个元素的汇总而形成的集体。 举个例子：给老王家送苹果，我想送10个苹果，一个一个的送会很麻烦。 怎么办呢，把10个苹果放在一个篮子中，拿着篮子取老王家。 例子中的篮子就是数组，苹果是数组中的一个一个元素。Java 中的数组是不可变的，像这个篮子一样制作完成之后篮子的大小就不能改变了。 初始化数组1234567int[] ints = {1, 2, 3, 4}; // 初始化方式 1String[] strings = new String[3]; // 初始化方式 2strings[0] = &quot;Hello&quot;; // 下标从 0 开始，向第 0 个位置添加字符串 Hellostrings[1] = &quot; &quot;; // 向第 1 个位置添加字符串 空格strings[2] = &quot;World&quot;; // 向第 2 个位置添加字符串 World// strings[3] = &quot;Hello&quot;; // 报错System.out.println(strings[0]); // 获取第 0 个元素并打印到控制台 包装类上述的8大基本数据类型都有对应的包装类(先了解，后面讲完对象之后会详细讲解。) byte -&gt; Byte short -&gt; Short int -&gt; Integer long -&gt; Long float -&gt; Float double -&gt; Double boolean -&gt; Boolean char -&gt; Character","link":"/posts/f42f2ee6.html"},{"title":"数据湖初步探索","text":"数据湖的概念数据湖是一个存储库，存储的数据可以是任意格式、结构化、非结构化、半结构化数据，允许对数据进行加工。数据湖从本质上来讲，是一种企业数据架构方法，物理上实现则是一个数据存储平台，用来集中化存储企业内海量的、多来源的、多种类的数据，并支持对数据进行快速加工和分析。 有了数据仓库为什么还要数据湖呢？数据仓库发展史： 离线数仓架构：所有数据通过Hive表存储在HDFS中，使用Hive SQL进行分析。分析的结果写到MySQL中，对外进行查询。 Lambda架构： 场景一：主要场景是离线分析场景，存在部分实时分析场景，离线分析 + 实时链路。 场景二：离线场景和实时场景都比较多，构建离线数仓 + 实时数仓。Lambda架构存在的问题： 离线数据和实时数仓同一个指标出来的数值存在不一致 业务逻辑需要开发两遍 分析数据需要存储两份 Kappa架构：离线和实时一套架构（不要离线的了）一般是公司在实时分析的场景使用比较多才会采用Kappa架构。Kappa架构架构存在问题： 所有中间分层数据存储在kafka中，无法使用SQL对分层数据进行即席查询。 Kafka中的数据存储是存在周期，并不是永久存储。 Kafka中的数据无法进行修改 无法复用目前已经非常成熟的基于离线数仓的数据血缘、数据质量管理体系。需要重新实现一套数据血缘、数据质量管理体系。 为了解决Kappa架构的痛点问题，业界最主流是采用“批流一体”方式，这里批流一体可以理解为批和流使用同一套SQL处理，也可以理解为处理框架的统一，例如：Spark、Flink，但这里更重要指的是存储层上的统一，只要存储层面上做到“批流一体”就可以解决以上Kappa遇到的各种问题。数据湖技术可以很好的实现存储层面上的“批流一体”，这就是为什么大数据中需要数据湖的原因。 数据湖与数据仓库的区别 存储数据类型方面数据仓库是存储数据，进行建模，存储的是结构化数据；数据湖以其本源格式保存大量原始数据，包括结构化的、半结构化的和非结构化的数据，主要是由原始的、混乱的、非结构化的数据组成。在需要数据之前，没有定义数据结构和需求。 数据处理模式在我们可以加载到数据仓库中的数据时，我们首先需要定义好它，这叫做写时模式（Schema-On-Write）。而对于数据湖，您只需加载原始数据，然后，当您准备使用数据时，就给它一个定义，这叫做读时模式（Schema-On-Read）。这是两种截然不同的数据处理方法。因为数据湖是在数据使用时再定义模型结构，因此提高了数据模型定义的灵活性，可满足更多不同上层业务的高效率分析诉求。 什么是 HudiHudi 是由Uber开发并开源的Data Lakes解决方案。Hudi能够基于HDFS之上管理大型分析数据集，可以对数据进行插入、更新、增量消费等操作，主要目的是高效减少摄取过程中的数据延迟。 Hudi 的特性 通过索引插件快速的删除和Upset。 来自 Spark、Presto、Trino、Hive 等的 SQL 读/写 事务、回滚、并发控制。 流式摄取、内置 CDC 源和工具。 自动调整文件大小、数据集群、压缩、清理。 用于可扩展存储访问的内置元数据跟踪。 向后兼容的模式演变和实施。 什么是 IcebergApache Iceberg是一种用于大型数据分析场景的开放表格式（Table Format）。Iceberg使用一种类似于SQL表的高性能表格式，Iceberg格式表单表可以存储数十PB数据，适配Spark、Trino、PrestoDB、Flink和Hive等计算引擎提供高性能的读写和元数据管理功能，Iceberg是一种数据湖解决方案。 Iceberg特性 Iceberg支持实时/批量数据写入和读取，支持Spark/Flink计算引擎。 Iceberg支持事务ACID,支持添加、删除、更新数据。 不绑定任何底层存储，支持Parquet、ORC、Avro格式兼容行存储和列存储。 Iceberg支持隐藏分区和分区变更，方便业务进行数据分区策略。 Iceberg支持快照数据重复查询，具备版本回滚功能。 Iceberg扫描计划很快，读取表或者查询文件可以不需要分布式SQL引擎。 Iceberg通过表元数据来对查询进行高效过滤。 基于乐观锁的并发支持，提供多线程并发写入能力并保证数据线性一致。","link":"/posts/89c758e.html"},{"title":"领域驱动设计-什么是领域","text":"领域驱动设计-什么是领域 – Eric Evans, Domain-Driven Design:Every software program relates to some activity or interest of its user. That subject area to which the user applies the program is the domain of the software. Some domains involve the physical world: The domain of an airline booking program involves real people getting on real aircraft. Some domains are intangible: the domain of an accounting program is money and finance. Software domains usually have little to do with computers, though there are exceptions: the domain of a source-code control system is software development itself. – Eric Evans, Domain-Driven Design. 翻译： 每个软件程序都与用户的某些活动或兴趣有关。用户应用该程序的主题领域就是该软件的领域。有些领域涉及物理世界。航空公司订票程序的领域涉及到真正的人登上真正的飞机。有些领域是无形的：一个会计程序的领域是金钱和财务。软件领域通常与计算机关系不大，但也有例外：源代码控制系统的领域就是软件开发本身。– 埃里克-埃文斯，领域驱动设计。 如何精炼业务域：领域愿景说明突出机制内聚机制分离的核心抽象核心","link":"/posts/d3721390.html"},{"title":"Kerberos常用命令","text":"通过keytab文件查找Principal 1klist -kte /etc/security/keytabs/flink.keytab 命令行kerberos认证： 1kinit -kt /etc/security/keytabs/flink_platform.service.keytab yarn/dn90210@DDP.COM ./bin/yarn-session.sh -d -jm 1024m -tm 2048 -s 1 bin/sql-gateway.sh start-foreground -D sql-gateway.endpoint.type=rest -D sql-gateway.endpoint.rest.address=localhost","link":"/posts/12990ea5.html"}],"tags":[],"categories":[{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Flink","slug":"Flink","link":"/categories/Flink/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"填写分类","slug":"填写分类","link":"/categories/%E5%A1%AB%E5%86%99%E5%88%86%E7%B1%BB/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"领域驱动设计","slug":"领域驱动设计","link":"/categories/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/"}],"pages":[]}