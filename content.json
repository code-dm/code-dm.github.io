{"posts":[{"title":"Hive Connector 简介","text":"本篇文章对应的Issues 本文基于：Flink-1.15 简介什么是Hive如果是了解过Hive以及Flink，这些简介请直接跳过。 Apache Hive官网的描述 The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive. Apache Hive是数据仓库的一种工具，用来读、写和管理分布式存储（例如：HDFS）中的大型的数据集。他可以把结构映射到存储的文件上。可以通过命令行工具和JDBC驱动来连接Hive。数据仓库收集企业中各个业务系统的数据进行集中化管理，Apache Hive可以作为数据仓库管理的一种工具。同时可以知道Hive本身是不存储数据的，他管理的是分布式存储系统中文件和结构的映射关系。例如：HDFS上有一个CSV文件，这个CSV文件使用 , 号进行分割 123a1,a2,a3b1,b2,b3c1,c2,c3 Apache Hive可以对这个CSV文件进行格式化，定义它的结构，例如定义分隔符、每一列的名称。定义完成后就可以使用SQL语句进行查询。这个过程可以看做是结构化的过程。 简介什么是Apache Flink相对于Apache Hive来说 Apache Flink则是Apache Hive的一个执行计算引擎。常见的Hive计算引擎有MapReduce、Spark，Flink也可以作为Hive的计算引擎。 Apache Flink官网的描述 Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. Apache Flink是对无界和有界数据流进行计算的一个框架和分布式处理的引擎。Flink可以以内存中的运算速度和任何的规模在常见的集群中运行。 Hive Connector的简介存储、计算、管理都是数据仓库生态中很重要的一部分。Hive有作为数据仓库生态中的核心，所以Flink针对Hive做了很多适配工作： 适配 Hive 的 MetaStore利用了 Hive 的 MetaStore 作为持久化的 Catalog，用户可通过HiveCatalog将不同会话中的 Flink 元数据存储到 Hive Metastore 中。 例如，用户可以使用HiveCatalog将其 Kafka 表或 Elasticsearch 表存储在 Hive Metastore 中，并后续在 SQL 查询中重新使用它们。 适配Hive方言，可以在Flink SQL-Client或者Flink SQL-Gateway中使用Hive的方言。 适配Hive读写HiveCatalog的设计提供了与 Hive 良好的兼容性，用户可以”开箱即用”的访问其已有的 Hive 数仓。 您不需要修改现有的 Hive Metastore，也不需要更改表的数据位置或分区。 适配Hive的函数 等等","link":"/posts/f3b4a32a.html"},{"title":"Hive Connector 实战使用","text":"本篇文章对应的Issues 本文基于：Flink-1.15 准备工作下载 Flink 安装包1wget https://dlcdn.apache.org/flink/flink-1.15.2/flink-1.15.2-bin-scala_2.12.tgz --no-check-certificate 下载 Hive 依赖包选择一个下载并放在 Flink 解压包的/lib/ 目录中。 Metastore version Maven dependency SQL Client JAR 1.0.0 - 1.2.2 flink-sql-connector-hive-1.2.2 Download 2.0.0 - 2.2.0 flink-sql-connector-hive-2.2.0 Download 2.3.0 - 2.3.6 flink-sql-connector-hive-2.3.6 Download 3.0.0 - 3.1.2 flink-sql-connector-hive-3.1.2 Download Hadoop 依赖1export HADOOP_CLASSPATH=`hadoop classpath` 配置 Kerberos如果 Hive 开启的 Kerberos 则进行配置，如果没有则跳过。在 conf/flink-conf.yam 配置文件中增加以下配置。 123security.kerberos.login.use-ticket-cache: truesecurity.kerberos.login.keytab: /keytabs/hive.service.keytabsecurity.kerberos.login.principal: hive 命令行执行： 1kinit -kt xxx.keytab xxxx 启动一个 Flin Standalone 环境为了可以远程访问 Flink Web界面，你可以将 conf/flink-conf.yam 中所有 localhost 全部替换成 0.0.0.0。增加指定端口配置： 1rest.port: 8081 启动集群： 1bin/start-cluster.sh 如果集群启动成功，通过浏览器可以访问Flink Web界面。注意Available Task Slots必须 &gt;= 1，如果为 0，则启动有异常。 连接 Hive Catalog1bin/sql-client.sh 12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'yyq', 'hive-conf-dir' = '/etc/hive/conf'); CREATE CATALOG的一些详细配置：输入 show databases; 可以显示 Hive 中所有的数据库。也可以写个 SELECT，sql-client会将作业提交到 Standalone 集群中，并返回查询结果。 参数 必选 默认值 类型 描述 type 是 (无) String Catalog 的类型。 创建 HiveCatalog 时，该参数必须设置为’hive’。 name 是 (无) String Catalog 的名字。仅在使用 YAML file 时需要指定。 hive-conf-dir 否 (无) String 指向包含 hive-site.xml 目录的 URI。 该 URI 必须是 Hadoop 文件系统所支持的类型。 如果指定一个相对 URI，即不包含 scheme，则默认为本地文件系统。如果该参数没有指定，我们会在 class path 下查找hive-site.xml。 default-database 否 default String 当一个catalog被设为当前catalog时，所使用的默认当前database。 hive-version 否 (无) String HiveCatalog 能够自动检测使用的 Hive 版本。我们建议不要手动设置 Hive 版本，除非自动检测机制失败。 hadoop-conf-dir 否 (无) String Hadoop 配置文件目录的路径。目前仅支持本地文件系统路径。我们推荐使用 HADOOP_CONF_DIR 环境变量来指定 Hadoop 配置。因此仅在环境变量不满足您的需求时再考虑使用该参数，例如当您希望为每个 HiveCatalog 单独设置 Hadoop 配置时。 从MySQL中同步数据到Hive123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051CREATE TABLE IF NOT EXISTS test_partition( id string comment '字段id注释', name string comment '字段name注释') COMMENT '备注表名' PARTITIONED BY( `dt` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( &quot;orc.compress&quot;=&quot;SNAPPY&quot;);set hive.exec.dynamic.partition.mode=nonstrict;insert into table yyq.test_partition partition(dt='2022-09-20')values('4','test_partition');CREATE TABLE IF NOT EXISTS yyq.test_stream_orc( id string comment 'id', name string comment 'name') COMMENT 'test_stream' ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' with serdeproperties('serialization.null.format' = '') STORED AS ORC TBLPROPERTIES( &quot;orc.compress&quot;=&quot;SNAPPY&quot;);insert into table test_stream_orc values('1','name1');CREATE TABLE `myhive`.`yyq`.`test_partition` ( `id` VARCHAR(2147483647), `name` VARCHAR(2147483647), `dt` VARCHAR(2147483647)) PARTITIONED BY (`dt`)WITH ( 'orc.compress' = 'SNAPPY', 'transient_lastDdlTime' = '1663580859', 'bucketing_version' = '2', 'connector' = 'hive')select * from test_partition /*+ OPTIONS('streaming-source.enable'='true','streaming-source.monitor-interval'='10s') */;","link":"/posts/e220ca62.html"},{"title":"Hive Connector读写源码解析","text":"本篇文章对应的IssuesFlink实现Hive中表数据的读写操作首先要获取到每张表的元数据，通过表的元数据信息才能获取到数据文件的存储路径、存储格式、是否分区、分区规则。这样才能从HDFS对应的路径中获取到对应的数据，并使用对应的存储格式来解析文件。Flink Hive的读写同样使用Connector通用架构新Source（FLIP-27）和Sink。 Flink 读取 Hive 配置信息Flink 支持以批和流两种模式从 Hive 表中读取数据。批读的时候，Flink 会基于执行查询时表的状态进行查询。流读时将持续监控表，并在表中新数据可用时进行增量获取，默认情况下，Flink 将以批模式读取数据。流读支持消费分区表和非分区表。对于分区表，Flink 会监控新分区的生成，并且在数据可用的情况下增量获取数据。对于非分区表，Flink 将监控文件夹中新文件的生成，并增量地读取新文件。 键 默认值 类型 描述 streaming-source.enable false Boolean 是否启动流读。注意：请确保每个分区/文件都应该原子地写入，否则读取不到完整的数据。 streaming-source.partition.include all String 选择读取的分区，可选项为 all 和 latest，all 读取所有分区；latest 读取按照 ‘streaming-source.partition.order’ 排序后的最新分区，latest 仅在流模式的 Hive 源表作为时态表时有效。默认的选项是 all。在开启 ‘streaming-source.enable’ 并设置 ‘streaming-source.partition.include’ 为 ‘latest’ 时，Flink 支持 temporal join 最新的 Hive 分区，同时，用户可以通过配置分区相关的选项来配置分区比较顺序和数据更新时间间隔。 streaming-source.monitor-interval None Duration 连续监控分区/文件的时间间隔。 注意: 默认情况下，流式读 Hive 的间隔为 ‘1 min’，但流读 Hive 的 temporal join 的默认时间间隔是 ‘60 min’，这是因为当前流读 Hive 的 temporal join 实现上有一个框架限制，即每个 TM 都要访问 Hive metastore，这可能会对 metastore 产生压力，这个问题将在未来得到改善。 streaming-source.partition-order partition-name String streaming source 分区排序，支持 create-time， partition-time 和 partition-name。 create-time 比较分区/文件创建时间， 这不是 Hive metastore 中创建分区的时间，而是文件夹/文件在文件系统的修改时间，如果分区文件夹以某种方式更新，比如添加在文件夹里新增了一个文件，它会影响到数据的使用。partition-time 从分区名称中抽取时间进行比较。partition-name 会比较分区名称的字典顺序。对于非分区的表，总是会比较 ‘create-time’。对于分区表默认值是 ‘partition-name’。该选项与已经弃用的 ‘streaming-source.consume-order’ 的选项相同 streaming-source.consume-start-offset None String 流模式起始消费偏移量。如何解析和比较偏移量取决于你指定的顺序。对于 create-time 和 partition-time，会比较时间戳 (yyyy-[m]m-[d]d [hh:mm:ss])。对于 partition-time，将使用分区时间提取器从分区名字中提取的时间。 对于 partition-name，是字符串类型的分区名称(比如 pt_year=2020/pt_mon=10/pt_day=01)。 Flink 读取 Hive 原理 HiveSource&lt;T&gt;继承了AbstractFileSource&lt;T, HiveSourceSplit&gt;类，AbstractFileSource又实现了核心的Source接口。Hive表中是不存储数据的，所有数据存储在HDFS中，所以HiveSource继承了AbstractFileSource抽象类。通过FLIP-27我们可以了解到Source类的作用： 一个工厂类 用来创建SplitEnumerator和SourceReader 创建用来生成CheckPoint的序列化类，以及从状态中恢复Enumerator HiveSourceBuilderorg.apache.flink.connectors.hive.HiveSourceBuilder类主要是通过配置信息构建HiveSource类。HiveSourceBuilder类中核心的方法：buildWithBulkFormat。 HiveSourceHiveSource构造方法中的参数： Path[] inputPaths: 文件系统中的一个目录或者一个文件，buildHiveSource传过来的参数是：new Path[1] FileEnumerator.Provider fileEnumerator: 一个可以创建HiveSourceFileEnumerator的工厂类 FileSplitAssigner.Provider splitAssigner: 创建FileSplitAssigner工厂类 BulkFormat&lt;T, HiveSourceSplit&gt; readerFormat: 首先了解什么是BulkFormat：BulkFormat是一个接口，BulkFormat 一次读取并解析一批记录。BulkFormat的实现包括ORC Format、Parquet Format、HiveInputFormat等。 外部的BulkFormat类主要充当reader的配置持有者和工厂角色(用来创建reader的工厂)。BulkFormat.Reader是在BulkFormat#createReader(Configuration, FileSourceSplit)方法中创建的，然后完成读取操作。如果在流的checkpoint执行期间基于checkpoint创建Bulk reader，那么reader是在BulkFormat#restoreReader(Configuration, FileSourceSplit)方法中重新创建的。 ContinuousEnumerationSettings continuousEnumerationSettings: 流式读取持续监控分区和文件的配置，包括监控的时间间隔。 如果是批量读取continuousEnumerationSettings为空，如果是流式读取continuousEnumerationSettings会被new出来。 int threadNum: 用来限制最大创建监控Hive分区和文件的线程数，在org.apache.flink.connectors.hive.MRSplitsGetter类中Executors.newFixedThreadPool(threadNum);限制线程池的数量。 从table.exec.hive.load-partition-splits.thread-num中获取的参数。 JobConf jobConf: 通过HiveConf转成的JobConf ObjectPath tablePath:table/view/function的名称 List&lt;String&gt; partitionKeys: 分区键 ContinuousPartitionFetcher&lt;Partition, ?&gt; fetcher: 是一个Hive分区获取器，可以根据previousOffset获取之后的分区。需要利用HiveContinuousPartitionFetcherContext-getComparablePartitionValueList获取所有可比较的分区列表。 HiveTableSource.HiveContinuousPartitionFetcherContext&lt;?&gt; fetcherContext: 从Hive的Meta Store中获取表的分区的上下文。HiveSource继承AbstractFileSource类，AbstractFileSource-createReader会创建一个核心类FileSourceReader，用来读取分布式文件系统中的文件。HiveSource-createEnumerator会创建核心类ContinuousHiveSplitEnumerator(流式读取)或调用父类createEnumerator方法创建StaticFileSplitEnumerator(批方式读取)类。 ContinuousHiveSplitEnumerator分区的发现分为初始化阶段和后续持续监控阶段，ContinuousHiveSplitEnumerator的作用是定时发现分布式文件系统中的分区。ContinuousHiveSplitEnumerator实现了SplitEnumerator类，程序会定时调用该类的start()方法用来监控分区。start()方法中enumeratorContext.callAsync(monitor, this::handleNewSplits, discoveryInterval, discoveryInterval)方法的入参： monitor：回调类，会定期调用该类的call()方法 this::handleNewSplits：传入的是一个函数。用来处理monitor发现的新分区 discoveryInterval：初始化延迟时间 discoveryInterval：周期延迟时间 PartitionMonitor#call()方法call()方法会获取表的所有分区，并过滤掉currentReadOffset位点之后的之前的旧分区。然后根据这些分区循环生成HiveSourceSplit，最终返回NewSplitsAndState。NewSplitsAndState中存储三个全局变量： T offset: 最新的offset Collection&lt;List&gt; seenPartitions: 已经处理过的分区offset Collection newSplits: 监控到的新分区，如果没有监控到则为空 StaticFileSplitEnumeratorFileSource批处理SplitEnumerator的具体实现。获取文件系统目录中所有文件并分配给Reader。HiveSource的批处理使用该类做处理。 FileSourceReader12345678910public FileSourceReader( SourceReaderContext readerContext, BulkFormat&lt;T, SplitT&gt; readerFormat, Configuration config) { super( () -&gt; new FileSourceSplitReader&lt;&gt;(config, readerFormat), new FileSourceRecordEmitter&lt;&gt;(), config, readerContext); } 核心作用在构造方法中向父类传入() -&gt; new FileSourceSplitReader&lt;&gt;(config, readerFormat)。真正调用读取逻辑的是FileSourceSplitReader FileSourceSplitReaderFileSourceSplitReader类中fetch方法调用批量读取方法： 123456789@Overridepublic RecordsWithSplitIds&lt;RecordAndPosition&lt;T&gt;&gt; fetch() throws IOException { checkSplitOrStartNext(); final BulkFormat.RecordIterator&lt;T&gt; nextBatch = currentReader.readBatch(); return nextBatch == null ? finishSplit() : FileRecords.forRecords(currentSplitId, nextBatch);} BulkFormat在HiveSource构造方法参数中有详细讲解过。","link":"/posts/76537a8.html"},{"title":"Hive Connector创建Catalog的原理","text":"本篇文章对应的Issues 本文基于：Flink-1.15 简介在sql-client中使用Hive Connector之前常规都是需要将Flink和Hive Metastore连接起来，从Hive Metastore中获取Hive的元数据，方便我们直接操作Hive中的一些表。同时我们在sql-client中创建对表也可以持久化在Hive Metastore中。使用Flink HiveCatalog可以连接到Hive Metastore，这边文章主要来了解HiveCatalog的实现。 HiveCatalog12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'mydatabase', 'hive-conf-dir' = '/opt/hive-conf'); 使用CREATE CATALOG语句Flink Table会根据type使用SPI找到对应的工厂类：HiveCatalogFactory。HiveCatalogFactory主要作用是获取相关配置，并创建HiveCatalog类。此处创建的Catalog会被管理在CatalogManager类中。HiveCatalog中的一些方法 getDatabase、createDatabase、alterDatabase、listDatabase、、、 getTable、createTable、renameTable、alterTable、dropTable、、、 partitionExists、createPartition、、、 createFunction、alterFunction、、、 alterTableStatistics、alterPartitionStatistics、、、 、、、 通过这些方法我们可以增删改查Hive中的数据库、表、视图以及函数等等。 HiveShimHiveCatalog并不是直接连接Hive Metastore的，而是通过 HiveShim 来连接到Hive Metastore的。HiveShim 的作用是处理Flink连接 Hive 在不同 Hive 版本版本中 Hive Metastore 不兼容的问题。 作业执行流程提交创建Catalog语句12345CREATE CATALOG myhive WITH ( 'type' = 'hive', 'default-database' = 'mydatabase', 'hive-conf-dir' = '/opt/hive-conf'); 用户通过 SQL Client 提交CREATE CATALOG SQL请求，Flink 会创建 TableEnvironment， TableEnvironment 会创建 CatalogManager 加载并配置 HiveCatalog 实例。初始化HiveCatalog会获取Hive的版本。后续会使用到。 提交USE语句1USE CATALOG myhive; CatalogManager 会修改全局currentCatalogName属性为myhive。 SHOW 语句1SHOW DATABASES; Flink Table 会调用HiveCatalog的listDatabases方法，Hive大版本不会存在差异的接口会使用IMetaStoreClient client直接获取getAllDatabases。 getHiveMetastoreClientgetHiveMetastoreClient在Hive各个大版本初始化存在差异，Flink使用HiveShim来处理不同版本的不同适配代码。例如：Hive1.0.0初始化HiveMetastoreClient代码： 12345678@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { return new HiveMetaStoreClient(hiveConf); } catch (MetaException ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }} Hive2.0.0初始化HiveMetastoreClient代码： 123456789101112131415161718192021222324@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { Class&lt;?&gt;[] constructorArgTypes = {HiveConf.class}; Object[] constructorArgs = {hiveConf}; Method method = RetryingMetaStoreClient.class.getMethod( &quot;getProxy&quot;, HiveConf.class, constructorArgTypes.getClass(), constructorArgs.getClass(), String.class); // getProxy is a static method return (IMetaStoreClient) method.invoke( null, hiveConf, constructorArgTypes, constructorArgs, HiveMetaStoreClient.class.getName()); } catch (Exception ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }} Hive3.1.0初始化HiveMetastoreClient代码： 123456789101112@Overridepublic IMetaStoreClient getHiveMetastoreClient(HiveConf hiveConf) { try { Method method = RetryingMetaStoreClient.class.getMethod( &quot;getProxy&quot;, Configuration.class, Boolean.TYPE); // getProxy is a static method return (IMetaStoreClient) method.invoke(null, hiveConf, true); } catch (Exception ex) { throw new CatalogException(&quot;Failed to create Hive Metastore client&quot;, ex); }}","link":"/posts/31388dbe.html"},{"title":"Docker安装MySQL","text":"确保已经安装Docker适用于 MySQL5.7 创建一个配置文件目录，并新建一个my.conf文件 执行命令123docker run -p 3306:3306 \\--name mysql8 -v 你创建的目录:/etc/mysql/conf.d \\-e MYSQL_ROOT_PASSWORD=123456 -d 镜像ID docker run -p 33261:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:tag –name some-mysql","link":"/posts/5c6bc735.html"},{"title":"什么是 Java","text":"本篇文章对应的Issues 维基百科中的Java 维基百科中的Java Java是一个高层次的(high-leve)、以类为基础的(class-based)、面向对象的(object-oriented)编程语言。Java的宗旨是一次编写，随处运行。也就是编译后的Java代码可以在所有支持Java的平台上运行，无需再重新编译。 Java 官网中的什么是Java https://www.java.com/download/help/whatis_java.html Java 发展历史","link":"/posts/9159529f.html"},{"title":"Java 安装教程","text":"本篇文章对应的Issues 下载JDK下载地址 下载最新版本JDK不用登录账号，但是下载JDK1.8需要登录账号，如果不想注册，可以使用我提供的账号（贴心不 :smirk: ）。 121414807686@qq.comCodedm96@codedm.com 下载页面往下拉可以看见Java8，根据您不同系统下载不同版本的JDK。 Windows安装JDK 双击打开安装包 点击下一步 点击下一步 点击下一步 设置中搜索 环境变量 -&gt; 点击编辑系统环境变量 -&gt; 点击环境变量 点击新建 新建Java Home 默认安装路径：C:\\Program Files\\Java\\ 下面的变量值建议到这个文件夹中复制，小版本号可能会存在差异。 变量名：JAVA_HOME变量值：C:\\Program Files\\Java\\jdk1.8.0_341 新建CLASSPATH 变量名：CLASSPATH变量值：.;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; 修改Path(注意是修改) 修改界面点击新建 新建两个： 12%JAVA_HOME%\\bin%JAVA_HOME%\\jre\\bin 所有窗口点击确定 MacOS安装JDK 下载完成之后双击打开，再双击黄色盒子 点击继续 -&gt; 点击安装 -&gt; 点击关闭 配置环境变量 打开终端输入下面命令 vim .zshrc 按下 i 键，再粘贴输入(注意修改成你自己的路径)： 1234567891011121314JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_341.jdk/Contents/HomePATH=$JAVA_HOME/bin:$PATH:.CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:.export JAVA_HOMEexport PATHexport CLASSPATH````&gt; 注意修改成你自己的版本号jdk1.8.0_341.jdk按键盘 esc -&gt; 输入 :wq -&gt; 回车再输入命令 source .zshrc 12345678910 &gt; 默认安装路径为： /Library/Java/JavaVirtualMachines/jdk1.8.0_341.jdk/Contents/Home## Linux安装JDK## 测试安装命令行输入： java -version 123显示出版本号则证明安装成功。 java version “1.8.0_341”Java(TM) SE Runtime Environment (build 1.8.0_341-b10)Java HotSpot(TM) 64-Bit Server VM (build 25.341-b10, mixed mode) 123命令行输入java以及javac均不报错： java 123以及 javac ```","link":"/posts/823e9fb.html"},{"title":"第一个 Java 程序 - Hello World","text":"本篇文章对应的Issues前两篇文章我们了解了什么是Java以及Java在各个平台的安装过程，这篇起，我们开始快乐的写Java代码，做一个Java程序员。 新建class文件首先随便找一个地方新建一个HelloWorld.java文件，然后用记事本打开他，Mac环境可以使用文本编辑器。然后一个字母一个字母打出下面的代码(不准复制。。)： 123456public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }} 命令行式运行在当前文件夹下打开命令行工具，Windows 打开CMD，Mac打开终端。输入命令 1javac HelloWorld.java 没有报错，没有出现异常。文件夹中出现HelloWorld.class文件，这是javac编译后的可运行文件。执行class文件： 1java HelloWorld 控制台会打印出HelloWorld，程序运行成功。 Idea开发工具运行下载安装Idea（略过，自己百度去），我还是给你们留了一个教程连接：安装教程安装教程创建完一个project，在project的src/main/java下创建一个类，命名HelloWorld。输入代码点击绿色箭头运行，下方控制台会出现我们程序要求打印的Hello World 代码讲解 注释常用注释两种 1234567单行注释 // 多行注释/** * 我的第一个Java程序 * * @author Code-dm */ 类（class）类名必须和文件名一样。暂时先理解为一个代码容器，后续的面向对象会详细讲解。 Main方法public static void main(String[] args)程序的主入口，一个程序只能有一个主入口。且每个程序都必须包含一个 main() 方法。 打印System.out.println()可以将字符串打印到控制台中。 System是Java核心库中的一个类 out是控制输出的对象 println()接收一个字符串并通过write输出到控制台","link":"/posts/6913f6d4.html"},{"title":"Java 的数据类型","text":"本篇文章对应的Issues 什么是变量变量的定义我们由一个停车场示例来讲解什么是变量。在C镇有很多停车场，小区停车场和公共停车场，停车场内根据不同大小的车辆划分了不同的停车位（大中小）。车辆停放必须根据车子的尺寸停放，先定义停车场的规则： 小区内的停车场只能停放该小区的车辆 小区外的公共停车场允许多个小区停放 车辆必须根据车子的尺寸严格停放到对应的车位中 将示例中各个角色和我们今天的变量对应停车场 -&gt; 变量 汽车 -&gt; 变量中的值公共停车场 —&gt; 成员变量小区内停车场 -&gt; 局部变量不同尺寸停车位 -&gt; Java中数据类型根据代码再来看看什么是变量： 1234567891011public class Variable { public static void main(String[] args) { int a; // 定义一个int类型的变量 a int x, y; // 定义两个int类型的变量 x 和 y int c = 1, d = 2; // 定义变量 c 赋值 1 和变量 d 赋值 2 a = 0; // 给 a 赋值 0 a = 1; // 把 a 的值变为 1 }} 从上面代码中可以看出定义一个变量首先要确定变量的类型，再给变量起个好听的名字，最后给变量赋值。变量为什么叫 “变”量，是因为他是可以改变的，也就是可以重新赋值。看例子。 12345678910public class Variable { public static void main(String[] args) { int i = 10; // 定义一个 int 类型变量并赋值为 10 System.out.println(i); // 打印 这个变量 输出 10 i = 11; // 重新赋值 11 System.out.println(i); // 打印 这个变量 输出 11 i 是可以重新赋值的 }} 变量的作用范围成员变量和局部变量最大的区别是作用范围不一样 1234567891011121314public class Variable { public static void main(String[] args) { int i = 10; if (true) { int b = 20; System.out.println(num); System.out.println(i); System.out.println(b); } // System.out.println(b); 编译报错 b 不在这个括号的作用范围内 }} 上述代码中 i 的作用范围在他所在大括号内b 的作用范围在他所在大括号内 i 的作用范围要比 b 的作用范围更广。 Java中基本数据类型 对应代码 -&gt; com.codedm.java.basic.datatype.PrimitiveDataType Java中有8种基本数据类型：byte存储从 -128 到 127 的整数 占用字节数： 1 byte 1byte b = 100; short存储从 -32,768 到 32,767 的整数 占用字节数： 2 byte 1short s = 100; int存储从 -2,147,483,648 到 2,147,483,647 的整数 占用字节数： 4 byte 1int t = 100; long存储从 -9,223,372,036,854,775,808 到 9,223,372,036,854,775,807 的整数 占用字节数： 8 byte 1long l = 100; float存储一个分数，能够储存6至7位的小数 占用字节数： 4 byte 1float f = 100.1f; double存储一个分数，能够储存15位的小数 占用字节数： 8 byte 1double d = 100.1; boolean存储一个 true 和 false 占用字节数： 1 bit 1boolean bl = true; char存储单个字符 占用字节数： 2 byte 1char c = 'C'; 从byte -> short -> int -> long 虽然存储的都是整数但是能够存储的位数是不一样的，位数越小占用的空间也就越小，选择合适的变量能够优化程序运行时占用的内存空间。 我们假设 ``1 byte`` 代表一个空格。 ![占用存储空间](https://codedm.oss-cn-hangzhou.aliyuncs.com/images/20220730/646efe8dcaa546f9a339774f1035ab66.png?x-oss-process=style/codedm) 虽然 ``int``也是存储 ``1`` 这个值，但是他占用了4个格子，即使格子是空的也会占用内存空间。 Java 中的引用数据类型除了8大基本数据类型其他的全部是引用数据类型 StringString 类型用来存储一段文本。(该类后面会详细讲解) 1String name = &quot;Codedm&quot;; 数组多个相同的数据类型组成的集合。 数学中集合的概念是：集合是指具有某种特定性质的具体的或抽象的对象汇总而成的集体。其中，构成集合的这些对象则称为该集合的元素。 数组和集合的概念类似：多个元素的汇总而形成的集体。 举个例子：给老王家送苹果，我想送10个苹果，一个一个的送会很麻烦。 怎么办呢，把10个苹果放在一个篮子中，拿着篮子取老王家。 例子中的篮子就是数组，苹果是数组中的一个一个元素。Java 中的数组是不可变的，像这个篮子一样制作完成之后篮子的大小就不能改变了。 初始化数组1234567int[] ints = {1, 2, 3, 4}; // 初始化方式 1String[] strings = new String[3]; // 初始化方式 2strings[0] = &quot;Hello&quot;; // 下标从 0 开始，向第 0 个位置添加字符串 Hellostrings[1] = &quot; &quot;; // 向第 1 个位置添加字符串 空格strings[2] = &quot;World&quot;; // 向第 2 个位置添加字符串 World// strings[3] = &quot;Hello&quot;; // 报错System.out.println(strings[0]); // 获取第 0 个元素并打印到控制台 包装类上述的8大基本数据类型都有对应的包装类(先了解，后面讲完对象之后会详细讲解。) byte -&gt; Byte short -&gt; Short int -&gt; Integer long -&gt; Long float -&gt; Float double -&gt; Double boolean -&gt; Boolean char -&gt; Character","link":"/posts/f42f2ee6.html"},{"title":"运算符和执行流程控制","text":"本篇文章对应的Issues Java中常用的的基本运算符数学运算符+-*/% 数学运算符和数学中运算符一样，使用的符号也是一样用。 递增和递减运算符++ 和 -- ++表示当前这个变量的值增加 1，-- 则相反，减 1 ，开发中使用最多的是 i++ 和 ++i，i 表示一个变量的名称。 12345678// i++ 和 ++i 的区别int i = 1;int j = ++i;System.out.println(j); // 2int x = 1;int y = x++;System.out.println(y); // 1 上述代码定义的 i 初始值为 1 ，然后再对 i 进行 ++i ，把 ++i 的值赋给 j ， j 最终打印出来的值为 2。下面的代码和下面的代码同理，只是变换成了 x++，打印出来的却是 1。拆分代码来理解， ++i 其实相当于 i = i + 1 然后再把 i 的值赋给 j，所以 j 的值是 2。x++ 相当于先把x 的值赋给 y，然后再执行 x = x + 1，所以 y 的值是 1 。 比较运算符&gt;&lt;&gt;=&lt;=instanceof比较运算符会返回 true 和 false 123// 比较运算符boolean t = 2 &gt; 1;System.out.println(); // true 逻辑运算符 &amp;&amp; ||&amp;&amp;||!&amp;&amp; 表示前后必须同时满足 true ，才是真的 true 。 123// &amp;&amp;System.out.println(2 &gt; 1 &amp;&amp; 3 &gt; 1); // trueSystem.out.println(2 &gt; 1 &amp;&amp; 0 &gt; 1); // false &amp; 和 &amp;&amp; 区别，我找了两篇文章可以看看： https://stackoverflow.com/questions/7199666/difference-between-and-in-java https://www.tutorialspoint.com/Differences-between-and-and-and-and-operators-in-Java 两篇文章都提到 &amp; 是按位运算符，&amp;&amp; 是合乎逻辑的。&amp; 评估的是给定数字的二进制的值，当 &amp; 开始运算时，它会从左边开始计算两个数字中字符的值。来个例子： 12System.out.println(10 &amp; 12);// returns 8 10 的二进制的值是 1010，12 的的二进制的值是 1100再开始运算之前我们需要知道一个小知识点： 1234System.out.println(1 &amp; 0); // 0System.out.println(0 &amp; 1); // 0System.out.println(1 &amp; 1); // 1System.out.println(0 &amp; 0); // 0 再来看看 &amp;&amp; ，它是按照逻辑推算的，它的逻辑规则是所有的表达式都是 true 整体表达式才是 true，只要运行到一个节点是false后面的就不会执行了。这被又称作短路。 || 表示前后有一个是 true 则返回 true 123// ||System.out.println(2 &gt; 1 || 3 &gt; 1); // trueSystem.out.println(2 &gt; 1 || 0 &gt; 1); // true ! 表示取反，意思是如果是 true 加上 ! 就是 false。 三元运算符三元运算符的使用场景一般是使用一个判断条件从两个选择中选择一个。 使用方法 condition ？ 选择一 : 选择二 如果 condition 是 true 则返回 选择一 ，如果是 false 则返回 选择二。 1234// 三元运算符int z = 2 &gt; 1 ? 10 : 11;System.out.println(z); // 10 2 &gt; 1 是 true，所以返回 10，则 z 的值为 10。 流程控制if 判断流程图中有一个菱形的图形，表示一个判断逻辑，会分出两个分支 是 分支和 否 分支。 if 语句和这个菱形的作用差不多。都是用来判断。 123if (判断条件) { // 如果判断条件是true，则执行大括号里面的代码。} 12345if (判断条件) { // 如果判断条件是true，则执行大括号里面的代码。} else { // 如果是false则执行这个大括号里面的代码。} 1234567if (判断条件1) { // 如果判断条件1是true，则执行大括号里面的代码。} else if (判断条件2) { // 如果判断条件2是true，则执行该大括号里面的代码。} else { // 如果上述判断条件都不满足则执行这个else大括号。} 1234567891011121314151617public class Judgment { public static void main(String[] args) { // 接收控制台输入 Scanner scanner = new Scanner(System.in); String input = scanner.nextLine(); // equals方法可以比较两个字符串是否一样 if (&quot;A&quot;.equals(input)) { System.out.println(&quot;AA&quot;); } else if (&quot;B&quot;.equals(input)) { System.out.println(&quot;BB&quot;); } else { System.out.println(&quot;CC&quot;); } }} SWITCH和 if 以及 if-else 不同的是，switch 是可以有多条执行路径的。java8中 switch 可以用byte、short、char 和 int原始数据类型来做判断。也支持枚举类型、String类型还有一些包装类。来个例子，需求是把数字1-12转成中文的1月-12月： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @author Codedm */public class SwitchDemo { public static void main(String[] args) { int month = 8; String monthString; switch (month) { case 1: monthString = &quot;一月&quot;; break; case 2: monthString = &quot;二月&quot;; break; case 3: monthString = &quot;三月&quot;; break; case 4: monthString = &quot;四月&quot;; break; case 5: monthString = &quot;五月&quot;; break; case 6: monthString = &quot;六月&quot;; break; case 7: monthString = &quot;七月&quot;; break; case 8: monthString = &quot;八月&quot;; break; case 9: monthString = &quot;九月&quot;; break; case 10: monthString = &quot;十月&quot;; break; case 11: monthString = &quot;十一月&quot;; break; case 12: monthString = &quot;十二月&quot;; break; default: monthString = &quot;无法匹配！&quot;; break; } System.out.println(monthString); }} 执行之后将会打印八月。现在需求变了，我需要根据输入数字的1-12，输出对应的季节。 12345678910111213141516171819202122232425262728293031323334353637/** * @author Codedm */public class SwitchSeason { public static void main(String[] args) { int month = 8; String monthString; switch (month) { case 1: case 2: case 12: monthString = &quot;冬季&quot;; break; case 3: case 4: case 5: monthString = &quot;春节&quot;; break; case 7: case 8: case 6: monthString = &quot;夏季&quot;; break; case 10: case 11: case 9: monthString = &quot;秋季&quot;; break; default: monthString = &quot;无法匹配！&quot;; break; } System.out.println(monthString); }} 当前这个程序会打印夏季。swich中的 default 表示上述的所有条件都不能满足或没有进行break会执行 default代码块。break的作用表示 switch 代码块中该 break 下面的代码不用执行了。 switch 相较于 if 拥有更强的可读性，如果有大量的 if 语句时会推荐使用 switch 来进行重构。 for循环假如我们想重复的输出一个字符串10次，如何实现呢： 123System.out.println(&quot;Hello World&quot;);System.out.println(&quot;Hello World&quot;);。。。 10行输出，这是一种实现方式（这方式也太蠢了）。使用for来解决。 12345678public class ControlFlow { public static void main(String[] args) { for (int i = 0; i &lt; 3; i++) { System.out.println(&quot;Hello World&quot;); } }} for循环中做了哪些： for() 括号中定义了一个变量 i 定义了 i 的循环条件是小于3的 每次循环后i都会进行 +1 for循环的执行流程：蓝色背景那行代码执行完成之后我们就已经初始化了 i 并给 i 赋值为 0，同时判断 0 是小于 3 的，符合循环条件，执行循环体（for() {}，{}大括号内的代码称为循环体），再执行 i++，由文章上面已经讲解了 i++ 是先进行赋值再进行自加 1，此时 i 变为 1。再跳到 for所在的那行，进行判断 i 值为 1 是小于 3 的继续执行循环体。直到 i 不再小于 3 就会执行 for 循环体后的代码。 for-eachwhile 和 do whilebreak 和 continue的区别","link":"/posts/2a65962b.html"},{"title":"Java中的面向对象OOP","text":"本篇文章对应的Issues面向对象OOP（Object-oriented programming） 面向过程 Process-oriented programming, POP开个网店本章节很重要，OOP是Java编程的核心思想，所以本章节我会尽力写的更加详细，用更多的例子来讲解。在讲Java的面向对象之前我们想讲讲为什么会有面向对象这个东西。在面向对象编程思想之前有一种面向过程(Process-oriented programming, POP)的编程思想，C语言其实就是面向过程的，面向过程的意思是我们按照事件发展的流程来写代码，每行代码对应事件发展的每个节点，直到事件结束，代码也就结束了。伴随着软件的发展，需求越来越复杂，代码也来越庞大，事件发展的分支越来越多，面向过程的弊端就凸显了，因为根本就没办法维护，修改一个小需求需要把整个过程走一遍。我们理解OOP的最好办法就是来看看面向过程在复杂场景为什么会这么吃力。文中使用案例来自：https://dins.site/coding-advanced-oop-motivation-chs/假设你是一个水果商人，你看到了互联网的发展给零售业带来的冲击和商机，于是你准备自己办一个网站，线上卖水果。由于你自己懂编程，所以你想自己写一个后台，把web端交给另一个人去做。这样即节省成本又有乐趣。于是你们两个约定好，web端给用户提供各种选择，其结果用二维数组的方式传给后台，后台根据文件计算水果价格收据，再返回给web端。这个故事发生在90年代，所以不需要考虑其他复杂的因素，把程序做出来就可以了。为了方便我们使用控制台的方式，通过控制台代替文本来传输各种选择给后端，后端计算价格在输出给控制台。 1234567891011121314151617181920212223242526272829303132333435/** * 面向过过程的程序设计 * * @author Codedm */public class PopDemo { public static void main(String[] args) { // 定义一个二维数组 存储水果序号、水果名称、水果单价 String[][] fruitData = { {&quot;苹果&quot;, &quot;2&quot;}, {&quot;橘子&quot;, &quot;5&quot;}, {&quot;香蕉&quot;, &quot;10&quot;}, {&quot;芒果&quot;, &quot;13&quot;}, {&quot;葡萄&quot;, &quot;15&quot;} }; System.out.println(&quot;请输入您需要购买水果的序号：&quot;); // 使用for循环打印各个水果名称和序号 for (int i = 0; i &lt; fruitData.length; i++) { System.out.println(&quot;序号：&quot; + i + &quot;，&quot; + fruitData[i][0]); } Scanner sc = new Scanner(System.in); int fruitNumber = sc.nextInt(); System.out.println(&quot;您选择的水果是：&quot; + fruitData[fruitNumber][0] + &quot;，它的单价是：&quot; + fruitData[fruitNumber][1]); System.out.println(&quot;请输入购买的重量：&quot;); // 此场景仅仅为了演示面向过程的编码风格，请忽略使用int类型来接收重量和价格。 int weight = sc.nextInt(); int totalPrice = Integer.parseInt(fruitData[fruitNumber][1]) * weight; System.out.println(&quot;总价为：&quot; + totalPrice); }} 吸引更多的顾客大家思考下现在我们是简单的计算价格的逻辑，渐渐的有更多的人来在线购买，同时为了吸引更多的用户，我们想发放优惠券。优惠券的逻辑还要增加在这个现有的逻辑里面。再比如还会有满减，配送费等等其他复杂的逻辑。 FOP为什么会失败？上述代码可以看出POP是紧耦合的，需求变更必然需要修改原来的代码。这点如果在代码不复杂的请看下看不出来，但是随着代码量越来越多问题就会越来越大。企业级的代码肯定不是几十行或者几百行可以完成的。OOP的出现其实就是从设计上来解决这种复杂代码的变更带来的维护问题。 面向对象OOP什么是类和对象在面向对象的编程中，类是来用创建对象的图纸、蓝图。通常来说一个类可以表示为一个人、一个地方或者一个事物，是一种抽象。比如说盖一个房子，房子的图纸（这张纸）是Java中的.java 文件，图纸上房子的架构就是类，根据这个图纸盖出来的房子是对象。对象是类的一个具象化的实体。我们可以拿这个图纸盖很多个房子，所以类只有一个，一个类可以构造出多个对象。 在Java中创建一个类","link":"/posts/fd4a56f1.html"},{"title":"填写标题","text":"本篇文章对应的Issues","link":"/posts/516f9a96.html"},{"title":"Kerberos常用命令","text":"通过keytab文件查找Principal 1klist -kte /etc/security/keytabs/flink.keytab 命令行kerberos认证： 1kinit -kt /etc/security/keytabs/flink_platform.service.keytab yarn/dn90210@DDP.COM ./bin/yarn-session.sh -d -jm 1024m -tm 2048 -s 1 bin/sql-gateway.sh start-foreground -D sql-gateway.endpoint.type=rest -D sql-gateway.endpoint.rest.address=localhost","link":"/posts/12990ea5.html"}],"tags":[],"categories":[{"name":"Flink","slug":"Flink","link":"/categories/Flink/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"填写分类","slug":"填写分类","link":"/categories/%E5%A1%AB%E5%86%99%E5%88%86%E7%B1%BB/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"pages":[]}